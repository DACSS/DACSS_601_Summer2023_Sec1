[
  {
    "objectID": "about/LinusJen.html",
    "href": "about/LinusJen.html",
    "title": "Linus Jen",
    "section": "",
    "text": "I graduated from UCLA in 2021 with a B.S. in statistics, and started my master’s at UMass right after graduation. I’ve been working with MassMutual for about 2 years now as a jr. data scientist."
  },
  {
    "objectID": "about/LinusJen.html#educationwork-background",
    "href": "about/LinusJen.html#educationwork-background",
    "title": "Linus Jen",
    "section": "",
    "text": "I graduated from UCLA in 2021 with a B.S. in statistics, and started my master’s at UMass right after graduation. I’ve been working with MassMutual for about 2 years now as a jr. data scientist."
  },
  {
    "objectID": "about/LinusJen.html#r-experience",
    "href": "about/LinusJen.html#r-experience",
    "title": "Linus Jen",
    "section": "R experience",
    "text": "R experience\nR was the first “programming” language I learned, and I used it extensively during undergrad. However, I haven’t touched R much since spring 2021, as I wanted to develop my Python skills. At one point in time, I’d say I was much more experienced with R, but I have forgotten a lot of things since then."
  },
  {
    "objectID": "about/LinusJen.html#research-interests",
    "href": "about/LinusJen.html#research-interests",
    "title": "Linus Jen",
    "section": "Research interests",
    "text": "Research interests\nMy interests mainly lie with data science overall - data wrangling, feature engineering and selection, and modeling. I do miss the interpretability and analysis aspect of stats (as black box models usually outperform linear models). I took COMPSCI 685 (Advanced NLP) this past semester, and would like to continue learning more about NLP as the field grows (though at this point, I don’t know anyone who doesn’t say they’re interested in NLP)."
  },
  {
    "objectID": "about/LinusJen.html#hometown",
    "href": "about/LinusJen.html#hometown",
    "title": "Linus Jen",
    "section": "Hometown",
    "text": "Hometown\nI’m from Monterey Park, CA (a suburb in LA), and lived there until moving here to Amherst."
  },
  {
    "objectID": "about/LinusJen.html#hobbies",
    "href": "about/LinusJen.html#hobbies",
    "title": "Linus Jen",
    "section": "Hobbies",
    "text": "Hobbies\nBasketball is my main passion in life, and when I’m back in LA, I play a lot of pickleball."
  },
  {
    "objectID": "about/LinusJen.html#fun-fact",
    "href": "about/LinusJen.html#fun-fact",
    "title": "Linus Jen",
    "section": "Fun fact",
    "text": "Fun fact\nMy family is named alphabetically starting from my oldest aunt (Anita) all the way down to me (Linus) and my sister (Megan)."
  },
  {
    "objectID": "about/AudreyBertin.html",
    "href": "about/AudreyBertin.html",
    "title": "Audrey Bertin",
    "section": "",
    "text": "Completed undergraduate degree in 2021 from Smith College in Statistical and Data Sciences (essentially, applied statistics) with a minor in Public Policy. Currently a masters student in Computer Science at UMass and hopefully graduating this summer!\nOn top of school, I work full time as a data scientist at MassMutual, primarily building predictive models in Python, where I’ve been since 2021."
  },
  {
    "objectID": "about/AudreyBertin.html#educationwork-background",
    "href": "about/AudreyBertin.html#educationwork-background",
    "title": "Audrey Bertin",
    "section": "",
    "text": "Completed undergraduate degree in 2021 from Smith College in Statistical and Data Sciences (essentially, applied statistics) with a minor in Public Policy. Currently a masters student in Computer Science at UMass and hopefully graduating this summer!\nOn top of school, I work full time as a data scientist at MassMutual, primarily building predictive models in Python, where I’ve been since 2021."
  },
  {
    "objectID": "about/AudreyBertin.html#r-experience",
    "href": "about/AudreyBertin.html#r-experience",
    "title": "Audrey Bertin",
    "section": "R experience",
    "text": "R experience\nUsed R in a few statistics/data science courses at Smith College so have some familiarity with tidyverse and some basic statistics functions/tests. However, it’s been a few years since I last used it regularly and there are lots of features I haven’t learned yet, both old things that I never learned before as well as new features like across()/quarto documents. I am hoping to both get a refresher on what I’ve forgotten as well as learn about some new functions and features I’ve not seen before. I’ve also primarily been doing machine learning the last few years so I’m excited for more experience with data visualization."
  },
  {
    "objectID": "about/AudreyBertin.html#research-interests",
    "href": "about/AudreyBertin.html#research-interests",
    "title": "Audrey Bertin",
    "section": "Research interests",
    "text": "Research interests\n\nHow to do “good” science (reproducible results, open access, etc)\nAlgorithmic bias\nPublic policy/law around data and responsible technology use"
  },
  {
    "objectID": "about/AudreyBertin.html#hometown",
    "href": "about/AudreyBertin.html#hometown",
    "title": "Audrey Bertin",
    "section": "Hometown",
    "text": "Hometown\nBorn and Raised in Austin, TX (until college). Just moved to Boston, MA!"
  },
  {
    "objectID": "about/AudreyBertin.html#hobbies",
    "href": "about/AudreyBertin.html#hobbies",
    "title": "Audrey Bertin",
    "section": "Hobbies",
    "text": "Hobbies\n\n35mm film photography\nElectric skateboarding\nVideo games and board games\nHiking/camping/outdoor stuff more generally\nRock climbing"
  },
  {
    "objectID": "about/AudreyBertin.html#fun-fact",
    "href": "about/AudreyBertin.html#fun-fact",
    "title": "Audrey Bertin",
    "section": "Fun fact",
    "text": "Fun fact\nI have many fun facts but one is that I am a former Texas state champion at competitive map reading."
  },
  {
    "objectID": "about/Zhongyue Lin.html",
    "href": "about/Zhongyue Lin.html",
    "title": "Zhongyue Lin",
    "section": "",
    "text": "My academic journey began with a dual degree in Finance and Business Management from the University of Nebraska Omaha. To further my understanding of human cognition and decision-making, I pursued a Master’s degree in Behavioral and Economic Sciences at the University of Warwick in the UK. This experience expanded my knowledge and offered deeper insights into inherent biases within these processes.\nDespite not having full-time work experience, I made the most of various internships and practical engagements during the gap year between my undergraduate and master’s degrees. This period proved instrumental for self-teaching in data analysis and programming. I served as a student assistant at a language lab and a data science teaching assistant at an online seminar, both experiences honing my research and teaching skills. I also participated in internships across the behavioral science consulting industry in the UK, social media data analysis in China, and as a quantitative financial analyst within the financial sector in China. These diverse experiences have not only honed my programming abilities but also cultivated a comprehensive understanding of data science and behavioral science, forming a robust foundation for my future research."
  },
  {
    "objectID": "about/Zhongyue Lin.html#educationwork-background",
    "href": "about/Zhongyue Lin.html#educationwork-background",
    "title": "Zhongyue Lin",
    "section": "",
    "text": "My academic journey began with a dual degree in Finance and Business Management from the University of Nebraska Omaha. To further my understanding of human cognition and decision-making, I pursued a Master’s degree in Behavioral and Economic Sciences at the University of Warwick in the UK. This experience expanded my knowledge and offered deeper insights into inherent biases within these processes.\nDespite not having full-time work experience, I made the most of various internships and practical engagements during the gap year between my undergraduate and master’s degrees. This period proved instrumental for self-teaching in data analysis and programming. I served as a student assistant at a language lab and a data science teaching assistant at an online seminar, both experiences honing my research and teaching skills. I also participated in internships across the behavioral science consulting industry in the UK, social media data analysis in China, and as a quantitative financial analyst within the financial sector in China. These diverse experiences have not only honed my programming abilities but also cultivated a comprehensive understanding of data science and behavioral science, forming a robust foundation for my future research."
  },
  {
    "objectID": "about/Zhongyue Lin.html#r-experience",
    "href": "about/Zhongyue Lin.html#r-experience",
    "title": "Zhongyue Lin",
    "section": "R experience",
    "text": "R experience\nI have taught myself R through Coursera and Udemy during my internship. Then I studied R for my first Masters at Warwick University, where I applied R to experimental analysis in behavioral science, psychology and cognitive science, but these experiences were entirely task-oriented and I did not learn R systematically."
  },
  {
    "objectID": "about/Zhongyue Lin.html#research-interests",
    "href": "about/Zhongyue Lin.html#research-interests",
    "title": "Zhongyue Lin",
    "section": "Research interests",
    "text": "Research interests\nMy research interest is to extend behavioral science from the micro level of the individual to the macro level of society. The human decision making process is influenced by the social environment, but if the decisions of a group of decision makers will in turn influence society? I find the relationship between the two interesting, and I hope that my studies in the DACSS program will help me to construct a connection between the two. And I hope to have the opportunity to apply my knowledge to research in the field of gerontology in the future."
  },
  {
    "objectID": "about/Zhongyue Lin.html#hometown",
    "href": "about/Zhongyue Lin.html#hometown",
    "title": "Zhongyue Lin",
    "section": "Hometown",
    "text": "Hometown\nShenzhen, China"
  },
  {
    "objectID": "about/Zhongyue Lin.html#hobbies",
    "href": "about/Zhongyue Lin.html#hobbies",
    "title": "Zhongyue Lin",
    "section": "Hobbies",
    "text": "Hobbies\n\nWatching movies (I’ve recently revisited Blade Runner and Blade Runner 2049!)\nFitness\nCooking\nListening to classical music (I am particularly fond of Ryuichi Sakamoto and Shostakovich)"
  },
  {
    "objectID": "about/Zhongyue Lin.html#fun-fact",
    "href": "about/Zhongyue Lin.html#fun-fact",
    "title": "Zhongyue Lin",
    "section": "Fun fact",
    "text": "Fun fact\n\nI was a student of film history before entering my undergraduate degree at UNO and I have seen 3000 films."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DACSS 601: Data Science Fundamentals",
    "section": "",
    "text": "The blog posts here are contributed by students enrolled in DACSS 601, Fundamentals of Data Science. The course provides students with an introduction to R and the tidyverse, scientific publishing, and collaboration through GitHub, building a foundation for future coursework. Students also are introduced to general data management and data wrangling skills, with an emphasis on best practice workflows and tidy data management.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nMay 31, 2023\n\n\nChallenge 1\n\n\nJocelyn Lutes\n\n\n\n\nMay 31, 2023\n\n\nChallenge 1 Submission\n\n\nMatt Zambetti\n\n\n\n\nMay 31, 2023\n\n\nChallenge 1 Submission\n\n\nSuyash Bhagwat\n\n\n\n\nMay 31, 2023\n\n\nChallenge 1 Instructions\n\n\nZhongyue Lin\n\n\n\n\nMay 30, 2023\n\n\nChallenge 1 Instructions\n\n\nSean Conway\n\n\n\n\nMay 30, 2023\n\n\nChallenge 8 Instructions\n\n\nSean Conway\n\n\n\n\nMay 30, 2023\n\n\nChallenge 2 Instructions\n\n\nSean Conway\n\n\n\n\nMay 30, 2023\n\n\nChallenge 5 Instructions\n\n\nSean Conway\n\n\n\n\nMay 30, 2023\n\n\nChallenge 6 Instructions\n\n\nSean Conway\n\n\n\n\nMay 30, 2023\n\n\nChallenge 4 Instructions\n\n\nSean Conway\n\n\n\n\nMay 30, 2023\n\n\nChallenge 9 Instructions\n\n\nSean Conway\n\n\n\n\nMay 30, 2023\n\n\nChallenge 3 Instructions\n\n\nSean Conway\n\n\n\n\nMay 30, 2023\n\n\nChallenge 1 Instructions\n\n\nSean Conway\n\n\n\n\nMay 30, 2023\n\n\nChallenge 10 Instructions\n\n\nSean Conway\n\n\n\n\nMay 30, 2023\n\n\nChallenge 7 Instructions\n\n\nSean Conway\n\n\n\n\nMay 30, 2023\n\n\nChallenge 1 Solution - Railroad Employment\n\n\nLinus Jen\n\n\n\n\nMay 30, 2023\n\n\nChallenge 1 - Reading and understanding bird data\n\n\nAudrey Bertin\n\n\n\n\nJun 5, 2022\n\n\nData Import\n\n\nSean Conway\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/challenge1_instructions.html",
    "href": "posts/challenge1_instructions.html",
    "title": "Challenge 1 Instructions",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge1_instructions.html#challenge-overview",
    "href": "posts/challenge1_instructions.html#challenge-overview",
    "title": "Challenge 1 Instructions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a dataset, and\ndescribe the dataset using both words and any supporting information (e.g., tables, etc)"
  },
  {
    "objectID": "posts/challenge1_instructions.html#read-in-the-data",
    "href": "posts/challenge1_instructions.html#read-in-the-data",
    "title": "Challenge 1 Instructions",
    "section": "Read in the Data",
    "text": "Read in the Data\nRead in one (or more) of the following data sets, using the correct R package and command.\n\nrailroad_2012_clean_county.csv ⭐\nbirds.csv ⭐⭐\nFAOstat*.csv ⭐⭐\nwild_bird_data.xlsx ⭐⭐⭐\nStateCounty2012.xls ⭐⭐⭐⭐\n\nFind the _data folder, located inside the posts folder. Then you can read in the data, using either one of the readr standard tidy read commands, or a specialized package such as readxl.\nAdd any comments or documentation as needed. More challenging data sets may require additional code chunks and documentation."
  },
  {
    "objectID": "posts/challenge1_instructions.html#describe-the-data",
    "href": "posts/challenge1_instructions.html#describe-the-data",
    "title": "Challenge 1 Instructions",
    "section": "Describe the data",
    "text": "Describe the data\nUsing a combination of words and results of R commands, can you provide a high level description of the data? Describe as efficiently as possible where/how the data was (likely) gathered, indicate the cases and variables (both the interpretation and any details you deem useful to the reader to fully understand your chosen data)."
  },
  {
    "objectID": "posts/challenge8_instructions.html",
    "href": "posts/challenge8_instructions.html",
    "title": "Challenge 8 Instructions",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggplot2)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge8_instructions.html#challenge-overview",
    "href": "posts/challenge8_instructions.html#challenge-overview",
    "title": "Challenge 8 Instructions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to:\n\nread in multiple data sets, and describe the data set using both words and any supporting information (e.g., tables, etc)\ntidy data (as needed, including sanity checks)\nmutate variables as needed (including sanity checks)\njoin two or more data sets and analyze some aspect of the joined data\n\n(be sure to only include the category tags for the data you use!)"
  },
  {
    "objectID": "posts/challenge8_instructions.html#read-in-data",
    "href": "posts/challenge8_instructions.html#read-in-data",
    "title": "Challenge 8 Instructions",
    "section": "Read in data",
    "text": "Read in data\nRead in one (or more) of the following datasets, using the correct R package and command.\n\nmilitary marriages ⭐⭐\nfaostat ⭐⭐\nrailroads ⭐⭐⭐\nfed_rate ⭐⭐⭐\ndebt ⭐⭐⭐\nus_hh ⭐⭐⭐⭐\nsnl ⭐⭐⭐⭐⭐\n\n\nBriefly describe the data"
  },
  {
    "objectID": "posts/challenge8_instructions.html#tidy-data-as-needed",
    "href": "posts/challenge8_instructions.html#tidy-data-as-needed",
    "title": "Challenge 8 Instructions",
    "section": "Tidy Data (as needed)",
    "text": "Tidy Data (as needed)\nIs your data already tidy, or is there work to be done? Be sure to anticipate your end result to provide a sanity check, and document your work here.\nAre there any variables that require mutation to be usable in your analysis stream? For example, do you need to calculate new values in order to graph them? Can string values be represented numerically? Do you need to turn any variables into factors and reorder for ease of graphics and visualization?\nDocument your work here."
  },
  {
    "objectID": "posts/challenge8_instructions.html#join-data",
    "href": "posts/challenge8_instructions.html#join-data",
    "title": "Challenge 8 Instructions",
    "section": "Join Data",
    "text": "Join Data\nBe sure to include a sanity check, and double-check that case count is correct!"
  },
  {
    "objectID": "posts/JocelynLutes_Challenge1.html",
    "href": "posts/JocelynLutes_Challenge1.html",
    "title": "Challenge 1",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/JocelynLutes_Challenge1.html#challenge-overview",
    "href": "posts/JocelynLutes_Challenge1.html#challenge-overview",
    "title": "Challenge 1",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a dataset, and\ndescribe the dataset using both words and any supporting information (e.g., tables, etc)"
  },
  {
    "objectID": "posts/JocelynLutes_Challenge1.html#read-in-the-data",
    "href": "posts/JocelynLutes_Challenge1.html#read-in-the-data",
    "title": "Challenge 1",
    "section": "Read in the Data",
    "text": "Read in the Data\nFor this challenge, I have chosen to read in the wild_bird_data.xlsx dataset. I chose to use this dataset because I have experience importing data from csv in R but have no experience importing Excel files. To read in the data, I utilized the read_excel function from the readxl library.\nThe raw xlsx file contains descriptive data in the first row of the file. Therefore, if we try to import the data using the default arguments of read_excel, the descriptive information is incorrectly assigned as the header row of the tibble. In order to import the data in the tabular form that we expect (header followed by the rows of data), we must set skip = 1 to skip over the first row. As shown in the sample below, this results in a tibble where the header is correctly assigned.\n\n\nCode\nwild_birds_data &lt;- read_excel('_data/wild_bird_data.xlsx', skip = 1)\nhead(wild_birds_data)\n\n\n# A tibble: 6 × 2\n  `Wet body weight [g]` `Population size`\n                  &lt;dbl&gt;             &lt;dbl&gt;\n1                  5.46           532194.\n2                  7.76          3165107.\n3                  8.64          2592997.\n4                 10.7           3524193.\n5                  7.42           389806.\n6                  9.12           604766."
  },
  {
    "objectID": "posts/JocelynLutes_Challenge1.html#describe-the-data",
    "href": "posts/JocelynLutes_Challenge1.html#describe-the-data",
    "title": "Challenge 1",
    "section": "Describe the data",
    "text": "Describe the data\n\nData Source\nBased on the first row of the raw xlsx file, we can see that this data was extracted from Figure 1 of a paper written by Nee et al. Although I was not able to obtain a copy of the paper to confirm, it seems possible that this data could have come from the paper “The relationship between abundance and body size in British birds”, which was published in Nature by Nee, Read, Greenwood, and Harvey in 1991.\nAssuming that this data was taken from Nee et al. (1991), then this data was collected to investigate the relationship between the size of birds and their population size.\n\n\nVariables\nThis dataset only contains two variables:\n\nWet Body Weight [g]: This is the weight in grams of the different bird species.\nPopulation Size: This is the size of the bird population. If the data was taken from Nee et al. (1991), the population sizes were published by the British Trust for Ornithology.\n\n\n\nDescriptive Statistics\nThis dataset contains data for (presumably) 146 species of birds, which is very similar to the 147 species that were included in the analyses by Nee et al. (1991).\n\n\nCode\npaste(\"Number of Observations:\", nrow(wild_birds_data))\n\n\n[1] \"Number of Observations: 146\"\n\n\nThe size of the birds in the sample varies greatly. The smallest bird species weighs just 5.46 grams and the largest species weighs 9639.85 grams. The average weight is 363.69 grams with a standard deviation of 983.55 grams.\n\n\nCode\n# calculate summary statistics for body weight\nwild_birds_data %&gt;%\n  rename(bw = `Wet body weight [g]`) %&gt;%\n  summarize(mean_weight = mean(bw), sd_weight = sd(bw), min_weight = min(bw), max_weight = max(bw))\n\n\n# A tibble: 1 × 4\n  mean_weight sd_weight min_weight max_weight\n        &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1        364.      984.       5.46      9640.\n\n\nThere is also considerable variation in the population sizes for the birds in the sample. Population varies from 4.9 to over 5 million! The mean population is 382,874 with a standard deviation of 951938.7.\n\n\nCode\n# calculate summary statistics for population size\nwild_birds_data %&gt;%\n  rename(pop = `Population size`) %&gt;%\n  summarize(mean_population = mean(pop), sd_population= sd(pop), min_population = min(pop), max_population = max(pop))\n\n\n# A tibble: 1 × 4\n  mean_population sd_population min_population max_population\n            &lt;dbl&gt;         &lt;dbl&gt;          &lt;dbl&gt;          &lt;dbl&gt;\n1         382874.       951939.           4.92       5093378.\n\n\nThe average weight (std dev) of the top 5 most populous birds is 47.27g (37.27g) , while the average weight (std dev) of the top 5 least populous birds is 96.3 g (154.37 g). Further analysis would be needed to determine if a meaningful relationship between body weight and population size exists in this dataset.\n\n\nCode\n# simple analysis to see how body weight differs based on population size\n\n# Most Populous Birds\nmost_pop &lt;- wild_birds_data %&gt;%\n  rename(pop = `Population size`, bw = `Wet body weight [g]`) %&gt;%\n  arrange(desc(pop)) %&gt;% \n  slice(1:5)\n\npaste('Average Weight of Top 5 Most Populous Birds:', round(mean(most_pop$bw), 2))\n\n\n[1] \"Average Weight of Top 5 Most Populous Birds: 47.27\"\n\n\nCode\npaste('Std Dev of Weight of Top 5 Most Populous Birds:', round(sd(most_pop$bw), 2))\n\n\n[1] \"Std Dev of Weight of Top 5 Most Populous Birds: 37.72\"\n\n\nCode\n# Least Populous Birds\nleast_pop &lt;- wild_birds_data %&gt;%\n  rename(pop = `Population size`, bw = `Wet body weight [g]`) %&gt;%\n  arrange(pop) %&gt;% \n  slice(1:5)\n\npaste('Average Weight of Top 5 Least Populous Birds:', round(mean(least_pop$bw), 2))\n\n\n[1] \"Average Weight of Top 5 Least Populous Birds: 96.3\"\n\n\nCode\npaste('Std Dev of Weight of Top 5 Least Populous Birds:', round(sd(least_pop$bw), 2))\n\n\n[1] \"Std Dev of Weight of Top 5 Least Populous Birds: 154.37\""
  },
  {
    "objectID": "posts/JocelynLutes_Challenge1.html#references",
    "href": "posts/JocelynLutes_Challenge1.html#references",
    "title": "Challenge 1",
    "section": "References",
    "text": "References\nNee, S., Read, A.F., Greenwood, J. J. D., & Harvey, P.H. (1991). The relationship between abundance and body size in British Birds, Nature, 351, 312-313."
  },
  {
    "objectID": "posts/challenge2_instructions.html",
    "href": "posts/challenge2_instructions.html",
    "title": "Challenge 2 Instructions",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge2_instructions.html#challenge-overview",
    "href": "posts/challenge2_instructions.html#challenge-overview",
    "title": "Challenge 2 Instructions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a data set, and describe the data using both words and any supporting information (e.g., tables, etc)\nprovide summary statistics for different interesting groups within the data, and interpret those statistics"
  },
  {
    "objectID": "posts/challenge2_instructions.html#read-in-the-data",
    "href": "posts/challenge2_instructions.html#read-in-the-data",
    "title": "Challenge 2 Instructions",
    "section": "Read in the Data",
    "text": "Read in the Data\nRead in one (or more) of the following data sets, available in the posts/_data folder, using the correct R package and command.\n\nrailroad*.csv or StateCounty2012.xls ⭐\nFAOstat*.csv or birds.csv ⭐⭐⭐\nhotel_bookings.csv ⭐⭐⭐⭐\n\nAdd any comments or documentation as needed. More challenging data may require additional code chunks and documentation."
  },
  {
    "objectID": "posts/challenge2_instructions.html#describe-the-data",
    "href": "posts/challenge2_instructions.html#describe-the-data",
    "title": "Challenge 2 Instructions",
    "section": "Describe the data",
    "text": "Describe the data\nUsing a combination of words and results of R commands, can you provide a high level description of the data? Describe as efficiently as possible where/how the data was (likely) gathered, indicate the cases and variables (both the interpretation and any details you deem useful to the reader to fully understand your chosen data)."
  },
  {
    "objectID": "posts/challenge2_instructions.html#provide-grouped-summary-statistics",
    "href": "posts/challenge2_instructions.html#provide-grouped-summary-statistics",
    "title": "Challenge 2 Instructions",
    "section": "Provide Grouped Summary Statistics",
    "text": "Provide Grouped Summary Statistics\nConduct some exploratory data analysis, using dplyr commands such as group_by(), select(), filter(), and summarise(). Find the central tendency (mean, median, mode) and dispersion (standard deviation, mix/max/quantile) for different subgroups within the data set.\n\nExplain and Interpret\nBe sure to explain why you choose a specific group. Comment on the interpretation of any interesting differences between groups that you uncover. This section can be integrated with the exploratory data analysis, just be sure it is included."
  },
  {
    "objectID": "posts/MattZambetti_challenge1.html",
    "href": "posts/MattZambetti_challenge1.html",
    "title": "Challenge 1 Submission",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/MattZambetti_challenge1.html#challenge-overview",
    "href": "posts/MattZambetti_challenge1.html#challenge-overview",
    "title": "Challenge 1 Submission",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a dataset, and\ndescribe the dataset using both words and any supporting information (e.g., tables, etc)"
  },
  {
    "objectID": "posts/MattZambetti_challenge1.html#read-in-the-data",
    "href": "posts/MattZambetti_challenge1.html#read-in-the-data",
    "title": "Challenge 1 Submission",
    "section": "Read in the Data",
    "text": "Read in the Data\nRead in one (or more) of the following data sets, using the correct R package and command.\n\nrailroad_2012_clean_county.csv ⭐\nbirds.csv ⭐⭐\nFAOstat*.csv ⭐⭐\nwild_bird_data.xlsx ⭐⭐⭐\nStateCounty2012.xls ⭐⭐⭐⭐\n\nFind the _data folder, located inside the posts folder. Then you can read in the data, using either one of the readr standard tidy read commands, or a specialized package such as readxl.\n\n\nCode\n# getting the data with the read.csv command\nrailroad_data &lt;- read.csv(\"_data/railroad_2012_clean_county.csv\")\n\n# printing the first 10 rows\nhead(railroad_data, 10)\n\n\n   state               county total_employees\n1     AE                  APO               2\n2     AK            ANCHORAGE               7\n3     AK FAIRBANKS NORTH STAR               2\n4     AK               JUNEAU               3\n5     AK    MATANUSKA-SUSITNA               2\n6     AK                SITKA               1\n7     AK SKAGWAY MUNICIPALITY              88\n8     AL              AUTAUGA             102\n9     AL              BALDWIN             143\n10    AL              BARBOUR               1"
  },
  {
    "objectID": "posts/MattZambetti_challenge1.html#describe-the-data",
    "href": "posts/MattZambetti_challenge1.html#describe-the-data",
    "title": "Challenge 1 Submission",
    "section": "Describe the data",
    "text": "Describe the data\nUsing a combination of words and results of R commands, can you provide a high level description of the data? Describe as efficiently as possible where/how the data was (likely) gathered, indicate the cases and variables (both the interpretation and any details you deem useful to the reader to fully understand your chosen data).\nFrom what we can see using the “head” command, there are three columns: state, county, and total number of employees for each railroad.\nI predict that the data was gathered either from on online registry with the department of labor or something similar, or this was done via survey. The survey would consist of the surveyor sending a request to each station to report how many employees there were at that particular station.\n\n\nCode\n# Lets sort the data\nsorted &lt;- railroad_data[order(railroad_data$total_employees, decreasing = TRUE),]\n\n# 10 stations with the most employees\nhead(sorted, 10)\n\n\n     state           county total_employees\n659     IL             COOK            8207\n2585    TX          TARRANT            4235\n1747    NE          DOUGLAS            3797\n1932    NY          SUFFOLK            3685\n2685    VA INDEPENDENT CITY            3249\n301     FL            DUVAL            3073\n195     CA   SAN BERNARDINO            2888\n178     CA      LOS ANGELES            2545\n2484    TX           HARRIS            2535\n1773    NE          LINCOLN            2289\n\n\nCode\n# 10 stations with the least employees\ntail(sorted, 10)\n\n\n     state    county total_employees\n2582    TX  STEPHENS               1\n2583    TX STONEWALL               1\n2609    TX   WILLACY               1\n2625    UT     GRAND               1\n2634    UT    SEVIER               1\n2691    VA LANCASTER               1\n2715    VA  RICHMOND               1\n2757    WA     FERRY               1\n2759    WA  GARFIELD               1\n2865    WV    GILMER               1\n\n\nThe two tables above, we can see the first contains the 10 stations with the most employees and in the second one, we can see the 10 stations with the least employees.\nThese two tables could be helpful for the challenge of distributing funds. For example, a station with more employees could be funded more since their payroll is higher than that of a station with fewer.\nAlso, stations with more employees is likely to be more populated leading to more wear and tear on the station. So, the upkeep of these stations is probably greater as well."
  },
  {
    "objectID": "posts/example-data_import.html",
    "href": "posts/example-data_import.html",
    "title": "Data Import",
    "section": "",
    "text": "Today, we’re going to read in three versions of the poultry_tidy data.\nWe will specifically read in 3 data files:\n- poultry_tidy.csv\n- poultry_tidy.xlsx\n- poultry_tidy.RData\nThese are the “clean” versions of the raw data files.\nTo run this file, all 3 datasets should be in the same directory on your computer."
  },
  {
    "objectID": "posts/example-data_import.html#overview",
    "href": "posts/example-data_import.html#overview",
    "title": "Data Import",
    "section": "",
    "text": "Today, we’re going to read in three versions of the poultry_tidy data.\nWe will specifically read in 3 data files:\n- poultry_tidy.csv\n- poultry_tidy.xlsx\n- poultry_tidy.RData\nThese are the “clean” versions of the raw data files.\nTo run this file, all 3 datasets should be in the same directory on your computer."
  },
  {
    "objectID": "posts/example-data_import.html#getting-started",
    "href": "posts/example-data_import.html#getting-started",
    "title": "Data Import",
    "section": "Getting Started",
    "text": "Getting Started\nTo begin, we need to load two packages: readr and readxl, which contain very useful functions for reading in data to `R.\nI’ll also load dplyr, one of the workhorse packages of tidyverse.\n\nlibrary(readr)\nlibrary(readxl)\nlibrary(dplyr)\n\nIf you’re unsure whether or not you have these packages installed, you can run the following command:\n\ninstalled.packages()\n\nWe’re now ready to get started reading in actual datasets."
  },
  {
    "objectID": "posts/example-data_import.html#reading-in-delimited-text-files",
    "href": "posts/example-data_import.html#reading-in-delimited-text-files",
    "title": "Data Import",
    "section": "Reading in delimited text files",
    "text": "Reading in delimited text files\n.csv is a common type of delimited text file. .csv stands for comma-separated value. This means that commas separate cells from one another.\nR has a base read.csv() function. However, it comes with a couple of downsides - namely that it imports data as a dataframe rather than a tibble. So we will be using the function read_csv() from the readr package. In addition to importing data as a tibble, it also does a much better job guessing data types.\nread_csv() is essentially a wrapper function (a function that calls another function) around the more general read_delim() function. Also see read_tsv() for tab-separated values.\n\n?read_delim\n\nLet’s look at the data files available for us to read in:\n\nlist.files(\"_data\")\n\n [1] \"~$poultry_tidy.xlsx\"                                                                             \n [2] \"AB_NYC_2019.csv\"                                                                                 \n [3] \"abc_poll_2021.csv\"                                                                               \n [4] \"ActiveDuty_MaritalStatus.xls\"                                                                    \n [5] \"animal_weight.csv\"                                                                               \n [6] \"australian_marriage_law_postal_survey_2017_-_response_final.xls\"                                 \n [7] \"australian_marriage_tidy.csv\"                                                                    \n [8] \"birds.csv\"                                                                                       \n [9] \"cereal.csv\"                                                                                      \n[10] \"cwc.csv\"                                                                                         \n[11] \"Data_Extract_From_World_Development_Indicators.xlsx\"                                             \n[12] \"Data_Extract_FromWorld Development Indicators.xlsx\"                                              \n[13] \"debt_in_trillions.xlsx\"                                                                          \n[14] \"eggs_tidy.csv\"                                                                                   \n[15] \"FAOSTAT_cattle_dairy.csv\"                                                                        \n[16] \"FAOSTAT_country_groups.csv\"                                                                      \n[17] \"FAOSTAT_egg_chicken.csv\"                                                                         \n[18] \"FAOSTAT_livestock.csv\"                                                                           \n[19] \"FedFundsRate.csv\"                                                                                \n[20] \"FRBNY-SCE-Public-Microdata-Complete-13-16.xlsx\"                                                  \n[21] \"hotel_bookings.csv\"                                                                              \n[22] \"organiceggpoultry.xls\"                                                                           \n[23] \"poultry_tidy.csv\"                                                                                \n[24] \"poultry_tidy.RData\"                                                                              \n[25] \"poultry_tidy.xlsx\"                                                                               \n[26] \"Public_School_Characteristics_2017-18.csv\"                                                       \n[27] \"railroad_2012_clean_county.csv\"                                                                  \n[28] \"sce-labor-chart-data-public.xlsx\"                                                                \n[29] \"snl_actors.csv\"                                                                                  \n[30] \"snl_casts.csv\"                                                                                   \n[31] \"snl_seasons.csv\"                                                                                 \n[32] \"starwars1.RData\"                                                                                 \n[33] \"StateCounty2012.xls\"                                                                             \n[34] \"test_objs.RData\"                                                                                 \n[35] \"Total_cost_for_top_15_pathogens_2018.xlsx\"                                                       \n[36] \"USA Households by Total Money Income, Race, and Hispanic Origin of Householder 1967 to 2019.xlsx\"\n[37] \"wild_bird_data.xlsx\"                                                                             \n\n\nThere’s a lot of data files there, but we are going to import the poultry_tidy.csv file. Doing so is very simple using read_csv():\n\npoultry_from_csv &lt;- read_csv(\"_data/poultry_tidy.csv\")\n\nRows: 600 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): Product, Month\ndbl (2): Year, Price_Dollar\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nLet’s take a look at our dataset (to view the tibble, running the name of the object will print it to the console):\n\npoultry_from_csv\n\n\n\n  \n\n\n\nIt worked great! The data is all there. To inspect the data types for each of the four columns in poultry_from_csv, we can use spec() or typeof():\n\nspec(poultry_from_csv) # use the spec() function to check the data type for your columns\n\ncols(\n  Product = col_character(),\n  Year = col_double(),\n  Month = col_character(),\n  Price_Dollar = col_double()\n)\n\n# can also use typeof() function on individual columns\ntypeof(poultry_from_csv$Product)\n\n[1] \"character\"\n\ntypeof(poultry_from_csv$Year)\n\n[1] \"double\"\n\ntypeof(poultry_from_csv$Month)\n\n[1] \"character\"\n\ntypeof(poultry_from_csv$Price_Dollar)\n\n[1] \"double\"\n\n\nSee this R section below for some more info on read_delim():\n\n# read_delim() has a number of optional arguments\nargs(read_delim)\n\nfunction (file, delim = NULL, quote = \"\\\"\", escape_backslash = FALSE, \n    escape_double = TRUE, col_names = TRUE, col_types = NULL, \n    col_select = NULL, id = NULL, locale = default_locale(), \n    na = c(\"\", \"NA\"), quoted_na = TRUE, comment = \"\", trim_ws = FALSE, \n    skip = 0, n_max = Inf, guess_max = min(1000, n_max), name_repair = \"unique\", \n    num_threads = readr_threads(), progress = show_progress(), \n    show_col_types = should_show_types(), skip_empty_rows = TRUE, \n    lazy = should_read_lazy()) \nNULL\n\n# there's too many to list here, so we will just go over a few\n# run ?read_delim() to learn more\n# 1) delim - text delimiter.\n# default is NULL and read_delim() guesses delimiter\n#\n# 2) quote - symbol telling R when to quote a string\n# default is \"\\\"\"\n# below comes from R documentation on quotes\n# https://stat.ethz.ch/R-manual/R-devel/library/base/html/Quotes.html\n# identical() is a function that returns TRUE if two objects are equal\nidentical(1+4, 3+2)\n\n[1] TRUE\n\nidentical('\"It\\'s alive!\", he screamed.',\n          \"\\\"It's alive!\\\", he screamed.\") # same\n\n[1] TRUE\n\n#\n# 3) escape_backlash\n# use backlash to escape special characters?\n# default = FALSE\n#\n# 4) col_names\n# can be TRUE (default), meaning that R reads in the first row of values as column names\n# can FALSE - R creates column names (x1 x2 etc)\n# OR can be a character vector of custom column names\npoultry_custom_cols &lt;- read_csv(\"_data/poultry_tidy.csv\",\n                                col_names = c(\"prod\",\"yr\",\"mo\",\"$\"),\n                                skip = 1) # need this to skip the file's column names\n\nRows: 600 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): prod, mo\ndbl (2): yr, $\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npoultry_custom_cols\n\n\n\n  \n\n\npoultry_custom_cols$`$` # note the backticks around the $ sign\n\n  [1] 2.38500 2.38500 2.38500 2.38500 2.38500 2.38500 2.38500 2.38500 2.38500\n [10] 2.38500 2.38500 2.38500 7.03750 7.03750 7.03750 7.03750 7.03750 7.03750\n [19] 7.03750 7.03750 7.03750 7.03750 7.03750 7.03750 3.90500 3.90500 3.90500\n [28] 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500\n [37] 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500\n [46] 2.03500 2.03500 2.03500 2.16250 2.16250 2.16250 2.16250 2.16250 2.16250\n [55] 2.16250 2.16250 2.16250 2.16250 2.16250 2.16250 2.35000 2.38500 2.38500\n [64] 2.38500 2.38500 2.38500 2.38500 2.38500 2.38500 2.38500 2.38500 2.38500\n [73] 6.37500 7.00000 7.00000 7.00000 7.00000 7.00000 7.00000 7.00000 7.00000\n [82] 7.00000 7.03750 7.03750 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500\n [91] 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 2.03500 2.03500 2.03500\n[100] 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500\n[109] 2.15000 2.15000 2.15000 2.15000 2.15000 2.15000 2.15000 2.16250 2.16250\n[118] 2.16250 2.16250 2.16250 2.35000 2.35000 2.35000 2.35000 2.35000 2.35000\n[127] 2.35000 2.35000 2.35000 2.35000 2.35000 2.35000 6.37500 6.37500 6.37500\n[136] 6.37500 6.37500 6.37500 6.37500 6.37500 6.37500 6.37500 6.37500 6.37500\n[145] 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500\n[154] 3.90500 3.90500 3.90500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500\n[163] 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.15000 2.15000 2.15000\n[172] 2.15000 2.15000 2.15000 2.15000 2.15000 2.15000 2.15000 2.15000 2.15000\n[181] 2.48000 2.48000 2.48000 2.41500 2.35000 2.35000 2.41500 2.35000 2.35000\n[190] 2.35000 2.35000 2.35000 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500\n[199] 6.45500 6.42300 6.37500 6.37500 6.37500 6.37500 3.90500 3.90500 3.90500\n[208] 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500\n[217] 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500\n[226] 2.03500 2.03500 2.03500 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000\n[235] 2.22000 2.19200 2.15000 2.15000 2.15000 2.15000 2.48000 2.48000 2.48000\n[244] 2.48000 2.48000 2.48000 2.48000 2.48000 2.48000 2.48000 2.48000 2.48000\n[253] 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500\n[262] 6.45500 6.45500 6.45500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500\n[271] 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 2.03500 2.03500 2.03500\n[280] 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500\n[289] 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000\n[298] 2.22000 2.22000 2.22000 2.20500 2.20500 2.20500 2.20500 2.20500 2.48000\n[307] 2.48000 2.48000 2.48000 2.48000 2.48000 2.48000 6.45500 6.45500 6.45500\n[316] 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500\n[325] 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500\n[334] 3.90500 3.90500 3.90500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500\n[343] 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.22000 2.22000 2.22000\n[352] 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000\n[361] 2.20500 2.20500 2.20500 2.20500 2.20500 2.20500 2.20500 2.20500 2.20500\n[370] 2.20500 2.20500 2.20500 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500\n[379] 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500 3.90500 3.90500 3.90500\n[388] 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500\n[397] 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500\n[406] 2.03500 2.03500 2.03500 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000\n[415] 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000 2.17000 2.17000 2.19625\n[424] 2.20500 2.20500 2.20500 2.20500 2.20500 2.20500 2.20500 2.20500 2.20500\n[433] 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500\n[442] 6.45500 6.45500 6.45500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500\n[451] 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 2.03500 2.03500 2.03500\n[460] 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500\n[469] 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000\n[478] 2.22000 2.22000 2.22000 2.17000 2.17000 2.17000 2.17000 2.17000 2.17000\n[487] 2.17000 2.17000 2.17000 2.17000 2.17000 2.17000 6.44000 6.45500 6.45500\n[496] 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500 6.45500\n[505] 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500\n[514] 3.90500 3.90500 3.90500 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500\n[523] 2.03500 2.03500 2.03500 2.03500 2.03500 2.03500 2.13000 2.22000 2.22000\n[532] 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000 2.22000\n[541] 1.97500 1.97500 2.09000 2.12000 2.14500 2.16375 2.17000 2.17000 2.17000\n[550] 2.17000 2.17000 2.17000 6.45500 6.42500 6.42500 6.42500 6.42500 6.41000\n[559] 6.42500 6.42500 6.42500 6.42500 6.42500 6.42500      NA      NA      NA\n[568]      NA      NA      NA 3.90500 3.90500 3.90500 3.90500 3.90500 3.90500\n[577] 1.93500 1.93500 1.93500 1.93500 1.93500 2.01875 2.03500 2.03500 2.03500\n[586] 2.03500 2.03500 2.03500      NA 2.03000 2.03000 2.03000 2.03000 2.00375\n[595] 1.99500 1.99500 1.99500 1.99500 1.99500 1.99500\n\n# $ is a \"special symbol\" in R, because it is an operator used for indexing\n# $ is technically an illegal column name, but we can still use it with ``\n# same goes for column names consisting of numbers or other symbols, etc.\n#\n# 5) col_types\n# default=NULL\n# if NULL R guesses data type from first 1000 rows\n# can also specify manually (but be careful)\n# see ?read_delim and scroll to col_types for details\n#\n# 6) skip\n# number of lines to skip\n# default=0\n# can be very useful with messy data files\n#\n# 7) n_max\n# maximum number of lines to read\n# default=Inf\n#\n#"
  },
  {
    "objectID": "posts/example-data_import.html#read-in-.xls.xlsx-files",
    "href": "posts/example-data_import.html#read-in-.xls.xlsx-files",
    "title": "Data Import",
    "section": "Read in .xls/.xlsx files",
    "text": "Read in .xls/.xlsx files\n.xls and .xlsx are files created in Microsoft Excel. There are separate functions read_xls() and read_xlsx(), but I find it’s best to use the wrapper function read_excel(). This will automatically call the correct function and avoid an error from accidentally mis-specifying the file type.\nSee below for what happens if we call the wrong function for the file type:\n\n# the try() function will try to run the code\n# see tryCatch() for more error handling \n# this code doesn't work because it tries to read the wrong file type\ntry(read_xls(\"_data/poultry_tidy.xlsx\"))\n\nError : \n  filepath: /Users/seanconway/Github/DACSS_601_Summer2023_Sec1/posts/_data/poultry_tidy.xlsx\n  libxls error: Unable to open file\n\n\nThe code below works just fine, however:\n\n# this code works \npoultry_from_excel &lt;- read_excel(\"_data/poultry_tidy.xlsx\")\npoultry_from_excel \n\n\n\n  \n\n\n\nLet’s take a look at this tibble:\n\n# examining our tibble\nhead(poultry_from_excel) # view the first several rows\n\n\n\n  \n\n\ncolnames(poultry_from_excel) # print column names\n\n[1] \"Product\"      \"Year\"         \"Month\"        \"Price_Dollar\"\n\nglimpse(poultry_from_excel) # tidy little summary of it\n\nRows: 600\nColumns: 4\n$ Product      &lt;chr&gt; \"Whole\", \"Whole\", \"Whole\", \"Whole\", \"Whole\", \"Whole\", \"Wh…\n$ Year         &lt;dbl&gt; 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 2013, 201…\n$ Month        &lt;chr&gt; \"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"…\n$ Price_Dollar &lt;dbl&gt; 2.3850, 2.3850, 2.3850, 2.3850, 2.3850, 2.3850, 2.3850, 2…\n\n# the package::function() syntax is only necessary if the package isn't loaded\n\nFunction documentation:\n\n# to view function documentation\n?read_excel\n\n# optional arguments\n# 1) sheet=NULL\n# number of the sheet to read in\n# by default it reads the first sheet\n\n# 2) range=NULL\n# range of cells to read in\n# uses the cellranger package to work with specific cells in Excel files\n# for more, see the cellranger package\n# https://cran.r-project.org/web/packages/cellranger/index.html\n\n# 3) col_names=TRUE\n# how to get column names (works the same as read_delim())\n\n# 4) col_types=NULL\n# types of data in columns (works the same as read_delim())\n\n# 5) skip = 0\n# number of lines to skip (works the same as read_delim())\n\n# 6) n_max=Inf\n# max lines to read (works the same as read_delim())"
  },
  {
    "objectID": "posts/example-data_import.html#reading-in-.rdata-files",
    "href": "posts/example-data_import.html#reading-in-.rdata-files",
    "title": "Data Import",
    "section": "Reading in .RData Files",
    "text": "Reading in .RData Files\nReading .RData is less commonly needed, but it’s still important to know about. .RData is a file type exclusively associated with R. It’s commonly used when someone has performed operations with data and saved the results to give to collaborators.\nWe can use the load() function to load R objects into our R environment from a file:\n\n# running the load() function on the data file name will load the objects into your R environment\nload(\"_data/poultry_tidy.RData\")\npoultry_tidy\n\n\n\n  \n\n\n# there's now a poultry_tidy object in our R environment\n\nNote that we do not assign the data file to an object. Rather, it comes in as an object based on whatever the previous user named it as. If we try to assign it as an object, the object will only have the name of the data file, rather than the data itself:\n\n# note that this operation shouldn't include any variable assignment\ntest_dat &lt;- load(\"_data/poultry_tidy.RData\")\ntest_dat # now it contains the object name, not the object itself\n\n[1] \"poultry_tidy\"\n\n\nYou can also save any number of R objects to a .RData file using the save() function:\n\na &lt;- rnorm(1000)\nb &lt;- matrix(runif(100),nrow=50,ncol=2)\nc &lt;- as_tibble(mtcars)\nsave(a,b,c,file=\"_data/test_objs.RData\")\n# there is now a test_objs.RData file in my working directory: \nlist.files(\"_data/\")\n\n [1] \"~$poultry_tidy.xlsx\"                                                                             \n [2] \"AB_NYC_2019.csv\"                                                                                 \n [3] \"abc_poll_2021.csv\"                                                                               \n [4] \"ActiveDuty_MaritalStatus.xls\"                                                                    \n [5] \"animal_weight.csv\"                                                                               \n [6] \"australian_marriage_law_postal_survey_2017_-_response_final.xls\"                                 \n [7] \"australian_marriage_tidy.csv\"                                                                    \n [8] \"birds.csv\"                                                                                       \n [9] \"cereal.csv\"                                                                                      \n[10] \"cwc.csv\"                                                                                         \n[11] \"Data_Extract_From_World_Development_Indicators.xlsx\"                                             \n[12] \"Data_Extract_FromWorld Development Indicators.xlsx\"                                              \n[13] \"debt_in_trillions.xlsx\"                                                                          \n[14] \"eggs_tidy.csv\"                                                                                   \n[15] \"FAOSTAT_cattle_dairy.csv\"                                                                        \n[16] \"FAOSTAT_country_groups.csv\"                                                                      \n[17] \"FAOSTAT_egg_chicken.csv\"                                                                         \n[18] \"FAOSTAT_livestock.csv\"                                                                           \n[19] \"FedFundsRate.csv\"                                                                                \n[20] \"FRBNY-SCE-Public-Microdata-Complete-13-16.xlsx\"                                                  \n[21] \"hotel_bookings.csv\"                                                                              \n[22] \"organiceggpoultry.xls\"                                                                           \n[23] \"poultry_tidy.csv\"                                                                                \n[24] \"poultry_tidy.RData\"                                                                              \n[25] \"poultry_tidy.xlsx\"                                                                               \n[26] \"Public_School_Characteristics_2017-18.csv\"                                                       \n[27] \"railroad_2012_clean_county.csv\"                                                                  \n[28] \"sce-labor-chart-data-public.xlsx\"                                                                \n[29] \"snl_actors.csv\"                                                                                  \n[30] \"snl_casts.csv\"                                                                                   \n[31] \"snl_seasons.csv\"                                                                                 \n[32] \"starwars1.RData\"                                                                                 \n[33] \"StateCounty2012.xls\"                                                                             \n[34] \"test_objs.RData\"                                                                                 \n[35] \"Total_cost_for_top_15_pathogens_2018.xlsx\"                                                       \n[36] \"USA Households by Total Money Income, Race, and Hispanic Origin of Householder 1967 to 2019.xlsx\"\n[37] \"wild_bird_data.xlsx\"                                                                             \n\n\nLet’s remove these objects from our R environment and re-load them from the file we saved:\n\n# remove objects from environment\nrm(list=c(\"a\",\"b\",\"c\"))\n\n# now they're back! (If you save them)\ntry(load(\"_data/test_objs.RData\"))"
  },
  {
    "objectID": "posts/example-data_import.html#conclusion",
    "href": "posts/example-data_import.html#conclusion",
    "title": "Data Import",
    "section": "Conclusion",
    "text": "Conclusion\nYou now know a little bit about how to read in some common data types. Note that these aren’t the only types of data you’ll encounter, but they are by far the most common ones."
  },
  {
    "objectID": "posts/challenge5_instructions.html",
    "href": "posts/challenge5_instructions.html",
    "title": "Challenge 5 Instructions",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggplot2)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge5_instructions.html#challenge-overview",
    "href": "posts/challenge5_instructions.html#challenge-overview",
    "title": "Challenge 5 Instructions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to:\n\nread in a data set, and describe the data set using both words and any supporting information (e.g., tables, etc)\ntidy data (as needed, including sanity checks)\nmutate variables as needed (including sanity checks)\ncreate at least two univariate visualizations\n\n\ntry to make them “publication” ready\nExplain why you choose the specific graph type\n\n\nCreate at least one bivariate visualization\n\n\ntry to make them “publication” ready\nExplain why you choose the specific graph type\n\nR Graph Gallery is a good starting point for thinking about what information is conveyed in standard graph types, and includes example R code.\n(be sure to only include the category tags for the data you use!)"
  },
  {
    "objectID": "posts/challenge5_instructions.html#read-in-data",
    "href": "posts/challenge5_instructions.html#read-in-data",
    "title": "Challenge 5 Instructions",
    "section": "Read in data",
    "text": "Read in data\nRead in one (or more) of the following datasets, using the correct R package and command.\n\ncereal.csv ⭐\nTotal_cost_for_top_15_pathogens_2018.xlsx ⭐\nAustralian Marriage ⭐⭐\nAB_NYC_2019.csv ⭐⭐⭐\nStateCounty2012.xls ⭐⭐⭐\nPublic School Characteristics ⭐⭐⭐⭐\nUSA Households ⭐⭐⭐⭐⭐\n\n\nBriefly describe the data"
  },
  {
    "objectID": "posts/challenge5_instructions.html#tidy-data-as-needed",
    "href": "posts/challenge5_instructions.html#tidy-data-as-needed",
    "title": "Challenge 5 Instructions",
    "section": "Tidy Data (as needed)",
    "text": "Tidy Data (as needed)\nIs your data already tidy, or is there work to be done? Be sure to anticipate your end result to provide a sanity check, and document your work here.\nAre there any variables that require mutation to be usable in your analysis stream? For example, do you need to calculate new values in order to graph them? Can string values be represented numerically? Do you need to turn any variables into factors and reorder for ease of graphics and visualization?\nDocument your work here."
  },
  {
    "objectID": "posts/challenge5_instructions.html#univariate-visualizations",
    "href": "posts/challenge5_instructions.html#univariate-visualizations",
    "title": "Challenge 5 Instructions",
    "section": "Univariate Visualizations",
    "text": "Univariate Visualizations"
  },
  {
    "objectID": "posts/challenge5_instructions.html#bivariate-visualizations",
    "href": "posts/challenge5_instructions.html#bivariate-visualizations",
    "title": "Challenge 5 Instructions",
    "section": "Bivariate Visualization(s)",
    "text": "Bivariate Visualization(s)\nAny additional comments?"
  },
  {
    "objectID": "posts/challenge6_instructions.html",
    "href": "posts/challenge6_instructions.html",
    "title": "Challenge 6 Instructions",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggplot2)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge6_instructions.html#challenge-overview",
    "href": "posts/challenge6_instructions.html#challenge-overview",
    "title": "Challenge 6 Instructions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to:\n\nread in a data set, and describe the data set using both words and any supporting information (e.g., tables, etc)\ntidy data (as needed, including sanity checks)\nmutate variables as needed (including sanity checks)\ncreate at least one graph including time (evolution)\n\n\ntry to make them “publication” ready (optional)\nExplain why you choose the specific graph type\n\n\nCreate at least one graph depicting part-whole or flow relationships\n\n\ntry to make them “publication” ready (optional)\nExplain why you choose the specific graph type\n\nR Graph Gallery is a good starting point for thinking about what information is conveyed in standard graph types, and includes example R code.\n(be sure to only include the category tags for the data you use!)"
  },
  {
    "objectID": "posts/challenge6_instructions.html#read-in-data",
    "href": "posts/challenge6_instructions.html#read-in-data",
    "title": "Challenge 6 Instructions",
    "section": "Read in data",
    "text": "Read in data\nRead in one (or more) of the following datasets, using the correct R package and command.\n\ndebt ⭐\nfed_rate ⭐⭐\nabc_poll ⭐⭐⭐\nusa_hh ⭐⭐⭐\nhotel_bookings ⭐⭐⭐⭐\nAB_NYC ⭐⭐⭐⭐⭐\n\n\nBriefly describe the data"
  },
  {
    "objectID": "posts/challenge6_instructions.html#tidy-data-as-needed",
    "href": "posts/challenge6_instructions.html#tidy-data-as-needed",
    "title": "Challenge 6 Instructions",
    "section": "Tidy Data (as needed)",
    "text": "Tidy Data (as needed)\nIs your data already tidy, or is there work to be done? Be sure to anticipate your end result to provide a sanity check, and document your work here.\nAre there any variables that require mutation to be usable in your analysis stream? For example, do you need to calculate new values in order to graph them? Can string values be represented numerically? Do you need to turn any variables into factors and reorder for ease of graphics and visualization?\nDocument your work here."
  },
  {
    "objectID": "posts/challenge6_instructions.html#time-dependent-visualization",
    "href": "posts/challenge6_instructions.html#time-dependent-visualization",
    "title": "Challenge 6 Instructions",
    "section": "Time Dependent Visualization",
    "text": "Time Dependent Visualization"
  },
  {
    "objectID": "posts/challenge6_instructions.html#visualizing-part-whole-relationships",
    "href": "posts/challenge6_instructions.html#visualizing-part-whole-relationships",
    "title": "Challenge 6 Instructions",
    "section": "Visualizing Part-Whole Relationships",
    "text": "Visualizing Part-Whole Relationships"
  },
  {
    "objectID": "posts/challenge4_instructions.html",
    "href": "posts/challenge4_instructions.html",
    "title": "Challenge 4 Instructions",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge4_instructions.html#challenge-overview",
    "href": "posts/challenge4_instructions.html#challenge-overview",
    "title": "Challenge 4 Instructions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to:\n\nread in a data set, and describe the data set using both words and any supporting information (e.g., tables, etc)\ntidy data (as needed, including sanity checks)\nidentify variables that need to be mutated\nmutate variables and sanity check all mutations"
  },
  {
    "objectID": "posts/challenge4_instructions.html#read-in-data",
    "href": "posts/challenge4_instructions.html#read-in-data",
    "title": "Challenge 4 Instructions",
    "section": "Read in data",
    "text": "Read in data\nRead in one (or more) of the following datasets, using the correct R package and command.\n\nabc_poll.csv ⭐\npoultry_tidy.xlsx or organiceggpoultry.xls⭐⭐\nFedFundsRate.csv⭐⭐⭐\nhotel_bookings.csv⭐⭐⭐⭐\ndebt_in_trillions.xlsx ⭐⭐⭐⭐⭐\n\n\nBriefly describe the data"
  },
  {
    "objectID": "posts/challenge4_instructions.html#tidy-data-as-needed",
    "href": "posts/challenge4_instructions.html#tidy-data-as-needed",
    "title": "Challenge 4 Instructions",
    "section": "Tidy Data (as needed)",
    "text": "Tidy Data (as needed)\nIs your data already tidy, or is there work to be done? Be sure to anticipate your end result to provide a sanity check, and document your work here.\nAny additional comments?"
  },
  {
    "objectID": "posts/challenge4_instructions.html#identify-variables-that-need-to-be-mutated",
    "href": "posts/challenge4_instructions.html#identify-variables-that-need-to-be-mutated",
    "title": "Challenge 4 Instructions",
    "section": "Identify variables that need to be mutated",
    "text": "Identify variables that need to be mutated\nAre there any variables that require mutation to be usable in your analysis stream? For example, are all time variables correctly coded as dates? Are all string variables reduced and cleaned to sensible categories? Do you need to turn any variables into factors and reorder for ease of graphics and visualization?\nDocument your work here.\nAny additional comments?"
  },
  {
    "objectID": "posts/challenge9_instructions.html",
    "href": "posts/challenge9_instructions.html",
    "title": "Challenge 9 Instructions",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggplot2)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge9_instructions.html#challenge-overview",
    "href": "posts/challenge9_instructions.html#challenge-overview",
    "title": "Challenge 9 Instructions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is simple. Create a function, and use it to perform a data analysis / cleaning / visualization task:\nExamples of such functions are: 1) A function that reads in and cleans a dataset.\n2) A function that computes summary statistics (e.g., computes the z score for a variable).\n3) A function that plots a histogram.\nThat’s it!"
  },
  {
    "objectID": "posts/SuyashBhagwat_challenge1.html",
    "href": "posts/SuyashBhagwat_challenge1.html",
    "title": "Challenge 1 Submission",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/SuyashBhagwat_challenge1.html#challenge-overview",
    "href": "posts/SuyashBhagwat_challenge1.html#challenge-overview",
    "title": "Challenge 1 Submission",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a dataset, and\ndescribe the dataset using both words and any supporting information (e.g., tables, etc)"
  },
  {
    "objectID": "posts/SuyashBhagwat_challenge1.html#read-in-the-data",
    "href": "posts/SuyashBhagwat_challenge1.html#read-in-the-data",
    "title": "Challenge 1 Submission",
    "section": "Read in the Data",
    "text": "Read in the Data\nRead in one (or more) of the following data sets, using the correct R package and command.\n\nrailroad_2012_clean_county.csv ⭐\nbirds.csv ⭐⭐\nFAOstat*.csv ⭐⭐\nwild_bird_data.xlsx ⭐⭐⭐\nStateCounty2012.xls ⭐⭐⭐⭐\n\nFind the _data folder, located inside the posts folder. Then you can read in the data, using either one of the readr standard tidy read commands, or a specialized package such as readxl.\n\n\nCode\n#Since this is my first time using R, I am using the easiest data set.\n#I am using the read_csv function to read the csv file\ndata_railroad &lt;- read_csv(\"_data/railroad_2012_clean_county.csv\")"
  },
  {
    "objectID": "posts/SuyashBhagwat_challenge1.html#describe-the-data",
    "href": "posts/SuyashBhagwat_challenge1.html#describe-the-data",
    "title": "Challenge 1 Submission",
    "section": "Describe the data",
    "text": "Describe the data\nUsing a combination of words and results of R commands, can you provide a high level description of the data? Describe as efficiently as possible where/how the data was (likely) gathered, indicate the cases and variables (both the interpretation and any details you deem useful to the reader to fully understand your chosen data).\nAns: The R code given below contains three common functions (spec(), head() and glimpse()) that are used to describe the data set.\nThe spec function tells us that the data set has three columns with the datatype given in brackets; state(chr),county(chr) and total_employees(dbl).\n\n\nCode\n#spec function gives an overall idea of the column variable types\nspec(data_railroad)\n\n\ncols(\n  state = col_character(),\n  county = col_character(),\n  total_employees = col_double()\n)\n\n\nThe head function gives us the first 6 rows.\n\n\nCode\n#Head function gives the first few rows of the data\nhead(data_railroad)\n\n\n# A tibble: 6 × 3\n  state county               total_employees\n  &lt;chr&gt; &lt;chr&gt;                          &lt;dbl&gt;\n1 AE    APO                                2\n2 AK    ANCHORAGE                          7\n3 AK    FAIRBANKS NORTH STAR               2\n4 AK    JUNEAU                             3\n5 AK    MATANUSKA-SUSITNA                  2\n6 AK    SITKA                              1\n\n\nThe glimpse function gives us the size of the data set which is 2930 rows x 3 cols. It also gives us an idea of the column names and datatypes along with the first few observations. Overall the glimpse function is the most concise method to get information about a data set.\nLooking at the data, it looks like it was gathered from the database of a railroad company in 2012 that keeps track of all its employees count by state and county.\n\n\nCode\nglimpse(data_railroad)\n\n\nRows: 2,930\nColumns: 3\n$ state           &lt;chr&gt; \"AE\", \"AK\", \"AK\", \"AK\", \"AK\", \"AK\", \"AK\", \"AL\", \"AL\", …\n$ county          &lt;chr&gt; \"APO\", \"ANCHORAGE\", \"FAIRBANKS NORTH STAR\", \"JUNEAU\", …\n$ total_employees &lt;dbl&gt; 2, 7, 2, 3, 2, 1, 88, 102, 143, 1, 25, 154, 13, 29, 45…"
  },
  {
    "objectID": "posts/challenge3_instructions.html",
    "href": "posts/challenge3_instructions.html",
    "title": "Challenge 3 Instructions",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge3_instructions.html#challenge-overview",
    "href": "posts/challenge3_instructions.html#challenge-overview",
    "title": "Challenge 3 Instructions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to:\n\nread in a data set, and describe the data set using both words and any supporting information (e.g., tables, etc)\nidentify what needs to be done to tidy the current data\nanticipate the shape of pivoted data\npivot the data into tidy format using pivot_longer"
  },
  {
    "objectID": "posts/challenge3_instructions.html#read-in-data",
    "href": "posts/challenge3_instructions.html#read-in-data",
    "title": "Challenge 3 Instructions",
    "section": "Read in data",
    "text": "Read in data\nRead in one (or more) of the following datasets, using the correct R package and command.\n\nanimal_weights.csv ⭐\neggs_tidy.csv ⭐⭐ or organiceggpoultry.xls ⭐⭐⭐\naustralian_marriage*.xls ⭐⭐⭐\nUSA Households*.xlsx ⭐⭐⭐⭐\nsce_labor_chart_data_public.xlsx 🌟🌟🌟🌟🌟\n\n\nBriefly describe the data\nDescribe the data, and be sure to comment on why you are planning to pivot it to make it “tidy”"
  },
  {
    "objectID": "posts/challenge3_instructions.html#anticipate-the-end-result",
    "href": "posts/challenge3_instructions.html#anticipate-the-end-result",
    "title": "Challenge 3 Instructions",
    "section": "Anticipate the End Result",
    "text": "Anticipate the End Result\nThe first step in pivoting the data is to try to come up with a concrete vision of what the end product should look like - that way you will know whether or not your pivoting was successful.\nOne easy way to do this is to think about the dimensions of your current data (tibble, dataframe, or matrix), and then calculate what the dimensions of the pivoted data should be.\nSuppose you have a dataset with \\(n\\) rows and \\(k\\) variables. In our example, 3 of the variables are used to identify a case, so you will be pivoting \\(k-3\\) variables into a longer format where the \\(k-3\\) variable names will move into the names_to variable and the current values in each of those columns will move into the values_to variable. Therefore, we would expect \\(n * (k-3)\\) rows in the pivoted dataframe!\n\nExample: find current and future data dimensions\nLets see if this works with a simple example.\n\n\nCode\ndf&lt;-tibble(country = rep(c(\"Mexico\", \"USA\", \"France\"),2),\n           year = rep(c(1980,1990), 3), \n           trade = rep(c(\"NAFTA\", \"NAFTA\", \"EU\"),2),\n           outgoing = rnorm(6, mean=1000, sd=500),\n           incoming = rlogis(6, location=1000, \n                             scale = 400))\ndf\n\n\n# A tibble: 6 × 5\n  country  year trade outgoing incoming\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 Mexico   1980 NAFTA     527.    1078.\n2 USA      1990 NAFTA    1201.    -607.\n3 France   1980 EU        558.     466.\n4 Mexico   1990 NAFTA     616.     954.\n5 USA      1980 NAFTA     348.     965.\n6 France   1990 EU        301.    1036.\n\n\nCode\n#existing rows/cases\nnrow(df)\n\n\n[1] 6\n\n\nCode\n#existing columns/cases\nncol(df)\n\n\n[1] 5\n\n\nCode\n#expected rows/cases\nnrow(df) * (ncol(df)-3)\n\n\n[1] 12\n\n\nCode\n# expected columns \n3 + 2\n\n\n[1] 5\n\n\nOr simple example has \\(n = 6\\) rows and \\(k - 3 = 2\\) variables being pivoted, so we expect a new dataframe to have \\(n * 2 = 12\\) rows x \\(3 + 2 = 5\\) columns.\n\n\nChallenge: Describe the final dimensions\nDocument your work here.\nAny additional comments?"
  },
  {
    "objectID": "posts/challenge3_instructions.html#pivot-the-data",
    "href": "posts/challenge3_instructions.html#pivot-the-data",
    "title": "Challenge 3 Instructions",
    "section": "Pivot the Data",
    "text": "Pivot the Data\nNow we will pivot the data, and compare our pivoted data dimensions to the dimensions calculated above as a “sanity” check.\n\nExample\n\n\nCode\ndf&lt;-pivot_longer(df, col = c(outgoing, incoming),\n                 names_to=\"trade_direction\",\n                 values_to = \"trade_value\")\ndf\n\n\n# A tibble: 12 × 5\n   country  year trade trade_direction trade_value\n   &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;                 &lt;dbl&gt;\n 1 Mexico   1980 NAFTA outgoing               527.\n 2 Mexico   1980 NAFTA incoming              1078.\n 3 USA      1990 NAFTA outgoing              1201.\n 4 USA      1990 NAFTA incoming              -607.\n 5 France   1980 EU    outgoing               558.\n 6 France   1980 EU    incoming               466.\n 7 Mexico   1990 NAFTA outgoing               616.\n 8 Mexico   1990 NAFTA incoming               954.\n 9 USA      1980 NAFTA outgoing               348.\n10 USA      1980 NAFTA incoming               965.\n11 France   1990 EU    outgoing               301.\n12 France   1990 EU    incoming              1036.\n\n\nYes, once it is pivoted long, our resulting data are \\(12x5\\) - exactly what we expected!\n\n\nChallenge: Pivot the Chosen Data\nDocument your work here. What will a new “case” be once you have pivoted the data? How does it meet requirements for tidy data?\nAny additional comments?"
  },
  {
    "objectID": "posts/challenge1_solution.html",
    "href": "posts/challenge1_solution.html",
    "title": "Challenge 1 Instructions",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge1_solution.html#challenge-overview",
    "href": "posts/challenge1_solution.html#challenge-overview",
    "title": "Challenge 1 Instructions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a dataset, and\ndescribe the dataset using both words and any supporting information (e.g., tables, etc)"
  },
  {
    "objectID": "posts/challenge1_solution.html#read-in-the-data",
    "href": "posts/challenge1_solution.html#read-in-the-data",
    "title": "Challenge 1 Instructions",
    "section": "Read in the Data",
    "text": "Read in the Data\nRead in one (or more) of the following data sets, using the correct R package and command.\n\nrailroad_2012_clean_county.csv ⭐\nbirds.csv ⭐⭐\nFAOstat*.csv ⭐⭐\nwild_bird_data.xlsx ⭐⭐⭐\nStateCounty2012.xls ⭐⭐⭐⭐\n\nFind the _data folder, located inside the posts folder. Then you can read in the data, using either one of the readr standard tidy read commands, or a specialized package such as readxl.\n\n\nCode\nlist.files(\"_data\")\n\n\n [1] \"AB_NYC_2019.csv\"                                                                                 \n [2] \"abc_poll_2021.csv\"                                                                               \n [3] \"ActiveDuty_MaritalStatus.xls\"                                                                    \n [4] \"animal_weight.csv\"                                                                               \n [5] \"australian_marriage_law_postal_survey_2017_-_response_final.xls\"                                 \n [6] \"australian_marriage_tidy.csv\"                                                                    \n [7] \"birds.csv\"                                                                                       \n [8] \"cereal.csv\"                                                                                      \n [9] \"cwc.csv\"                                                                                         \n[10] \"Data_Extract_From_World_Development_Indicators.xlsx\"                                             \n[11] \"Data_Extract_FromWorld Development Indicators.xlsx\"                                              \n[12] \"debt_in_trillions.xlsx\"                                                                          \n[13] \"eggs_tidy.csv\"                                                                                   \n[14] \"FAOSTAT_cattle_dairy.csv\"                                                                        \n[15] \"FAOSTAT_country_groups.csv\"                                                                      \n[16] \"FAOSTAT_egg_chicken.csv\"                                                                         \n[17] \"FAOSTAT_livestock.csv\"                                                                           \n[18] \"FedFundsRate.csv\"                                                                                \n[19] \"FRBNY-SCE-Public-Microdata-Complete-13-16.xlsx\"                                                  \n[20] \"hotel_bookings.csv\"                                                                              \n[21] \"organiceggpoultry.xls\"                                                                           \n[22] \"poultry_tidy.csv\"                                                                                \n[23] \"poultry_tidy.RData\"                                                                              \n[24] \"poultry_tidy.xlsx\"                                                                               \n[25] \"Public_School_Characteristics_2017-18.csv\"                                                       \n[26] \"railroad_2012_clean_county.csv\"                                                                  \n[27] \"sce-labor-chart-data-public.xlsx\"                                                                \n[28] \"snl_actors.csv\"                                                                                  \n[29] \"snl_casts.csv\"                                                                                   \n[30] \"snl_seasons.csv\"                                                                                 \n[31] \"starwars1.RData\"                                                                                 \n[32] \"StateCounty2012.xls\"                                                                             \n[33] \"test_objs.RData\"                                                                                 \n[34] \"Total_cost_for_top_15_pathogens_2018.xlsx\"                                                       \n[35] \"USA Households by Total Money Income, Race, and Hispanic Origin of Householder 1967 to 2019.xlsx\"\n[36] \"wild_bird_data.xlsx\"                                                                             \n\n\n\n\nCode\nrailroad_csv &lt;- read_csv(\"_data/railroad_2012_clean_county.csv\") \nbirds_from_csv &lt;- read_csv(\"_data/birds.csv\")\nrailroad_csv \n\n\n# A tibble: 2,930 × 3\n   state county               total_employees\n   &lt;chr&gt; &lt;chr&gt;                          &lt;dbl&gt;\n 1 AE    APO                                2\n 2 AK    ANCHORAGE                          7\n 3 AK    FAIRBANKS NORTH STAR               2\n 4 AK    JUNEAU                             3\n 5 AK    MATANUSKA-SUSITNA                  2\n 6 AK    SITKA                              1\n 7 AK    SKAGWAY MUNICIPALITY              88\n 8 AL    AUTAUGA                          102\n 9 AL    BALDWIN                          143\n10 AL    BARBOUR                            1\n# ℹ 2,920 more rows\n\n\nCode\nbirds_from_csv\n\n\n# A tibble: 30,977 × 14\n   `Domain Code` Domain     `Area Code` Area  `Element Code` Element `Item Code`\n   &lt;chr&gt;         &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n 1 QA            Live Anim…           2 Afgh…           5112 Stocks         1057\n 2 QA            Live Anim…           2 Afgh…           5112 Stocks         1057\n 3 QA            Live Anim…           2 Afgh…           5112 Stocks         1057\n 4 QA            Live Anim…           2 Afgh…           5112 Stocks         1057\n 5 QA            Live Anim…           2 Afgh…           5112 Stocks         1057\n 6 QA            Live Anim…           2 Afgh…           5112 Stocks         1057\n 7 QA            Live Anim…           2 Afgh…           5112 Stocks         1057\n 8 QA            Live Anim…           2 Afgh…           5112 Stocks         1057\n 9 QA            Live Anim…           2 Afgh…           5112 Stocks         1057\n10 QA            Live Anim…           2 Afgh…           5112 Stocks         1057\n# ℹ 30,967 more rows\n# ℹ 7 more variables: Item &lt;chr&gt;, `Year Code` &lt;dbl&gt;, Year &lt;dbl&gt;, Unit &lt;chr&gt;,\n#   Value &lt;dbl&gt;, Flag &lt;chr&gt;, `Flag Description` &lt;chr&gt;\n\n\nAdd any comments or documentation as needed. More challenging data sets may require additional code chunks and documentation."
  },
  {
    "objectID": "posts/challenge1_solution.html#describe-the-data",
    "href": "posts/challenge1_solution.html#describe-the-data",
    "title": "Challenge 1 Instructions",
    "section": "Describe the data",
    "text": "Describe the data\nUsing a combination of words and results of R commands, can you provide a high level description of the data? Describe as efficiently as possible where/how the data was (likely) gathered, indicate the cases and variables (both the interpretation and any details you deem useful to the reader to fully understand your chosen data).\n\n\nCode\nglimpse(railroad_csv) \n\n\nRows: 2,930\nColumns: 3\n$ state           &lt;chr&gt; \"AE\", \"AK\", \"AK\", \"AK\", \"AK\", \"AK\", \"AK\", \"AL\", \"AL\", …\n$ county          &lt;chr&gt; \"APO\", \"ANCHORAGE\", \"FAIRBANKS NORTH STAR\", \"JUNEAU\", …\n$ total_employees &lt;dbl&gt; 2, 7, 2, 3, 2, 1, 88, 102, 143, 1, 25, 154, 13, 29, 45…\n\n\nCode\nspec(railroad_csv)\n\n\ncols(\n  state = col_character(),\n  county = col_character(),\n  total_employees = col_double()\n)\n\n\nCode\ntypeof(railroad_csv$state)\n\n\n[1] \"character\"\n\n\nCode\ntypeof(railroad_csv$county)\n\n\n[1] \"character\"\n\n\nCode\ntypeof(railroad_csv$total_employees)\n\n\n[1] \"double\"\n\n\nIn the railroad_2012_clean_county dataset we have 2930 rows and 3 columns(state, county and total_employees). With the data given in this dataset we can only know about the total number of employees in a county is and what state that county belongs to. And the types of the variables state, county and total_employees are character, character and double.\n\n\nCode\nglimpse(birds_from_csv) \n\n\nRows: 30,977\nColumns: 14\n$ `Domain Code`      &lt;chr&gt; \"QA\", \"QA\", \"QA\", \"QA\", \"QA\", \"QA\", \"QA\", \"QA\", \"QA…\n$ Domain             &lt;chr&gt; \"Live Animals\", \"Live Animals\", \"Live Animals\", \"Li…\n$ `Area Code`        &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ Area               &lt;chr&gt; \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afgha…\n$ `Element Code`     &lt;dbl&gt; 5112, 5112, 5112, 5112, 5112, 5112, 5112, 5112, 511…\n$ Element            &lt;chr&gt; \"Stocks\", \"Stocks\", \"Stocks\", \"Stocks\", \"Stocks\", \"…\n$ `Item Code`        &lt;dbl&gt; 1057, 1057, 1057, 1057, 1057, 1057, 1057, 1057, 105…\n$ Item               &lt;chr&gt; \"Chickens\", \"Chickens\", \"Chickens\", \"Chickens\", \"Ch…\n$ `Year Code`        &lt;dbl&gt; 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 196…\n$ Year               &lt;dbl&gt; 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 196…\n$ Unit               &lt;chr&gt; \"1000 Head\", \"1000 Head\", \"1000 Head\", \"1000 Head\",…\n$ Value              &lt;dbl&gt; 4700, 4900, 5000, 5300, 5500, 5800, 6600, 6290, 630…\n$ Flag               &lt;chr&gt; \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", \"F\", NA, \"F\", \"F\", \"F…\n$ `Flag Description` &lt;chr&gt; \"FAO estimate\", \"FAO estimate\", \"FAO estimate\", \"FA…\n\n\nCode\nspec(birds_from_csv) # To check data types of columns \n\n\ncols(\n  `Domain Code` = col_character(),\n  Domain = col_character(),\n  `Area Code` = col_double(),\n  Area = col_character(),\n  `Element Code` = col_double(),\n  Element = col_character(),\n  `Item Code` = col_double(),\n  Item = col_character(),\n  `Year Code` = col_double(),\n  Year = col_double(),\n  Unit = col_character(),\n  Value = col_double(),\n  Flag = col_character(),\n  `Flag Description` = col_character()\n)\n\n\nCode\ntypeof(birds_from_csv$'Domain Code') \n\n\n[1] \"character\"\n\n\nCode\ntypeof(birds_from_csv$Domain) \n\n\n[1] \"character\"\n\n\nCode\ntypeof(birds_from_csv$'Area Code') \n\n\n[1] \"double\"\n\n\nCode\ntypeof(birds_from_csv$Area) \n\n\n[1] \"character\"\n\n\nCode\ntypeof(birds_from_csv$'Element Code') \n\n\n[1] \"double\"\n\n\nCode\ntypeof(birds_from_csv$Element) \n\n\n[1] \"character\"\n\n\nCode\ntypeof(birds_from_csv$'Item Code') \n\n\n[1] \"double\"\n\n\nCode\ntypeof(birds_from_csv$Item) \n\n\n[1] \"character\"\n\n\nCode\ntypeof(birds_from_csv$'Year Code') \n\n\n[1] \"double\"\n\n\nCode\ntypeof(birds_from_csv$Year) \n\n\n[1] \"double\"\n\n\nCode\ntypeof(birds_from_csv$Unit) \n\n\n[1] \"character\"\n\n\nCode\ntypeof(birds_from_csv$Value) \n\n\n[1] \"double\"\n\n\nCode\ntypeof(birds_from_csv$Flag) \n\n\n[1] \"character\"\n\n\nCode\ntypeof(birds_from_csv$'Flag Description')\n\n\n[1] \"character\"\n\n\nIn the birds dataset we have 30977 rows and 14 columns(Domain, Area, Element, Year etc). In this dataset , the data is recorded form 1961 - 2018 , about the region(Area) and different types of birds(like chicken and duck) in that region and information about birds like count(Unit), Value and flag. This information is liekly gathered to know the inforamtion of number of birds in each area for every year from 1961 to 2018 or it may also be the information of buying live animals(birds) every year from each area."
  },
  {
    "objectID": "posts/ZhongyueLin_challenge1.html",
    "href": "posts/ZhongyueLin_challenge1.html",
    "title": "Challenge 1 Instructions",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\n# Import the \"readxl\" package\nlibrary(readxl)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/ZhongyueLin_challenge1.html#challenge-overview",
    "href": "posts/ZhongyueLin_challenge1.html#challenge-overview",
    "title": "Challenge 1 Instructions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a dataset, and\ndescribe the dataset using both words and any supporting information (e.g., tables, etc)"
  },
  {
    "objectID": "posts/ZhongyueLin_challenge1.html#read-in-the-data",
    "href": "posts/ZhongyueLin_challenge1.html#read-in-the-data",
    "title": "Challenge 1 Instructions",
    "section": "Read in the Data",
    "text": "Read in the Data\nRead in one (or more) of the following data sets, using the correct R package and command.\n\nwild_bird_data.xlsx ⭐⭐⭐\nStateCounty2012.xls ⭐⭐⭐⭐\n\nIn this challenge 1, I have chosen the raw data files “wild_bird_data.xlsx” and “StateCounty2012.xls” for practice. Since I have not had prior experience with handling and importing xls and xlsx data formats in R, importing these two raw data files is a new challenge for me.\n\nwild_bird_data\nWhile working with the data from “wild_bird_data.xlsx,” I encountered a situation where the header of the dataset occupied one row. To avoid any data format errors during the import process, I used the setting “skip=1” to skip the first row while importing the data.\n\n\nCode\n# Read the file\ndata_wild_brid &lt;- read_excel(\"_data/wild_bird_data.xlsx\",skip = 1)\n#Preview Data\nhead(data_wild_brid)\n\n\n# A tibble: 6 × 2\n  `Wet body weight [g]` `Population size`\n                  &lt;dbl&gt;             &lt;dbl&gt;\n1                  5.46           532194.\n2                  7.76          3165107.\n3                  8.64          2592997.\n4                 10.7           3524193.\n5                  7.42           389806.\n6                  9.12           604766.\n\n\nCode\ndim(data_wild_brid)\n\n\n[1] 146   2\n\n\nThen, I used the dim() function and head() function to have an initial preview of the data (146 rows, 2 columns).\n\n\nStateCounty2012\nIn the data import process, I made adjustments and expansions to the code based on the format of the original file. I skipped the first three rows, which contained the headers, and removed any “NA” values present in the dataset. I also removed any unrelated content at the end of the table.\nHowever, there are still some remaining issues with this dataset. Each letter code includes rows labeled as “total,” which can affect data aggregation and analysis. However, this issue can be addressed in subsequent data processing steps.\n\n\nCode\n# Read the file, skipping the first 3 rows and specifying column names, then remove all columns that are completely empty, then replace NA values in the \"COUNTY\" column with \"/\" when \"STATE\" contains \"Total\"\ndata_clean &lt;- read_excel(\"_data/StateCounty2012.xls\", skip = 3) %&gt;%\n  select_if(~!all(is.na(.))) %&gt;%\n  mutate(COUNTY = ifelse(is.na(COUNTY) & grepl(\"Total\", STATE), \"/\", COUNTY))\n\n# Print out the first few lines of the cleaned data\nhead(data_clean,100)\n\n\n# A tibble: 100 × 3\n   STATE     COUNTY               TOTAL\n   &lt;chr&gt;     &lt;chr&gt;                &lt;dbl&gt;\n 1 AE        APO                      2\n 2 AE Total1 /                        2\n 3 AK        ANCHORAGE                7\n 4 AK        FAIRBANKS NORTH STAR     2\n 5 AK        JUNEAU                   3\n 6 AK        MATANUSKA-SUSITNA        2\n 7 AK        SITKA                    1\n 8 AK        SKAGWAY MUNICIPALITY    88\n 9 AK Total  /                      103\n10 AL        AUTAUGA                102\n# ℹ 90 more rows\n\n\nCode\n# Identify the rows where \"STATE\" is \"CANADA\" and replace NA values in \"COUNTY\" with \"Canada\", then remove empty rows above the \"CANADA\" row\ndata_clean &lt;- data_clean %&gt;%\n  mutate(COUNTY = replace(COUNTY, STATE == \"CANADA\", \"Canada\")) %&gt;%\n  slice(1:(nrow(.)-4))\n\n# Print out the last few lines of the cleaned data to check\ntail(data_clean,100)\n\n\n# A tibble: 100 × 3\n   STATE COUNTY      TOTAL\n   &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;\n 1 WI    ROCK          138\n 2 WI    RUSK           21\n 3 WI    SAUK           29\n 4 WI    SAWYER         14\n 5 WI    SHAWANO         9\n 6 WI    SHEBOYGAN       9\n 7 WI    ST CROIX      168\n 8 WI    TAYLOR         15\n 9 WI    TREMPEALEAU    54\n10 WI    VERNON         48\n# ℹ 90 more rows\n\n\nCode\n# Remove the row before the \"CANADA\" row\ndata_clean &lt;- data_clean %&gt;%\n  slice(-which(.$STATE == \"CANADA\") + 1)\n\n# Print out the last few lines of the cleaned data to check\ntail(data_clean,100)\n\n\n# A tibble: 100 × 3\n   STATE COUNTY      TOTAL\n   &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;\n 1 WI    RACINE        100\n 2 WI    ROCK          138\n 3 WI    RUSK           21\n 4 WI    SAUK           29\n 5 WI    SAWYER         14\n 6 WI    SHAWANO         9\n 7 WI    SHEBOYGAN       9\n 8 WI    ST CROIX      168\n 9 WI    TAYLOR         15\n10 WI    TREMPEALEAU    54\n# ℹ 90 more rows\n\n\nIn the data import process, I made adjustments and expansions to the code based on the original file format. I skipped the first three rows, which contained the headers, and removed any “NA” values. However, there are still some issues with this dataset. Each letter code includes a “total” row, which can affect data summarization when performing calculations. This problem can be addressed in subsequent data processing steps. ## Describe the data Using a combination of words and results of R commands, can you provide a high level description of the data? Describe as efficiently as possible where/how the data was (likely) gathered, indicate the cases and variables (both the interpretation and any details you deem useful to the reader to fully understand your chosen data).\n\n\nwild_bird_data\n\n\nCode\n# Print the structure of the data\nstr(data_wild_brid)\n\n\ntibble [146 × 2] (S3: tbl_df/tbl/data.frame)\n $ Wet body weight [g]: num [1:146] 5.46 7.76 8.64 10.69 7.42 ...\n $ Population size    : num [1:146] 532194 3165107 2592997 3524193 389806 ...\n\n\nCode\n# Print a summary of the data\nsummary(data_wild_brid)\n\n\n Wet body weight [g] Population size  \n Min.   :   5.459    Min.   :      5  \n 1st Qu.:  18.620    1st Qu.:   1821  \n Median :  69.232    Median :  24353  \n Mean   : 363.694    Mean   : 382874  \n 3rd Qu.: 309.826    3rd Qu.: 198515  \n Max.   :9639.845    Max.   :5093378  \n\n\nFirstly, the dataset file does not provide specific background information on data collection, so the following description is based on assumptions from the data itself:\nThe dataset, named “wild_bird_data,” consists of 146 rows and 2 columns. This dataset is likely gathered from field research, where researchers possibly conducted bird weight measurements at a specific location or a series of locations, and recorded the bird population size in these areas.\nThe two main variables are “Wet body weight [g]” and “Population size”:\n“Wet body weight [g]”: This variable represents the wet body weight of birds (in grams). The values range from 5.459g to 9639.845g, with an average of about 363.694g, and a median of about 69.232g. This variable could be very useful in studying the health conditions of birds, their life habits, or the impact of environmental changes on bird weight.\n“Population size”: This variable represents the bird population size in a specific area. The values range widely, from a minimum of 5 to a maximum of 5093378, with an average of about 382874, and a median of about 24353. This variable could be very useful in studying the distribution of bird populations, dynamic changes in population, or the impact of environmental changes on population size.\n\n\nStateCounty2012\n\n\nCode\nstr(data_clean)\n\n\ntibble [2,985 × 3] (S3: tbl_df/tbl/data.frame)\n $ STATE : chr [1:2985] \"AE\" \"AE Total1\" \"AK\" \"AK\" ...\n $ COUNTY: chr [1:2985] \"APO\" \"/\" \"ANCHORAGE\" \"FAIRBANKS NORTH STAR\" ...\n $ TOTAL : num [1:2985] 2 2 7 2 3 2 1 88 103 102 ...\n\n\nCode\n# Remove rows where \"STATE\" contains \"total\"\ndata_clean_rm &lt;- data_clean %&gt;%\n  filter(!grepl(\"total\", STATE, ignore.case = TRUE))\nsummary(data_clean_rm)\n\n\n    STATE              COUNTY              TOTAL        \n Length:2931        Length:2931        Min.   :   1.00  \n Class :character   Class :character   1st Qu.:   7.00  \n Mode  :character   Mode  :character   Median :  21.00  \n                                       Mean   :  87.37  \n                                       3rd Qu.:  65.00  \n                                       Max.   :8207.00  \n\n\nCode\ntail(data_clean_rm,100)\n\n\n# A tibble: 100 × 3\n   STATE COUNTY  TOTAL\n   &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;\n 1 WI    PIERCE     43\n 2 WI    POLK       23\n 3 WI    PORTAGE   240\n 4 WI    PRICE      10\n 5 WI    RACINE    100\n 6 WI    ROCK      138\n 7 WI    RUSK       21\n 8 WI    SAUK       29\n 9 WI    SAWYER     14\n10 WI    SHAWANO     9\n# ℹ 90 more rows\n\n\nCode\n# Find the row with the maximum value\nmax_row &lt;- data_clean %&gt;%\n  filter(TOTAL == max(TOTAL, na.rm = TRUE))\n\n# Print out the row with the maximum value\nprint(max_row)\n\n\n# A tibble: 1 × 3\n  STATE       COUNTY  TOTAL\n  &lt;chr&gt;       &lt;chr&gt;   &lt;dbl&gt;\n1 Grand Total /      255432\n\n\nCode\n# Find the row with the minimum value\nmin_row &lt;- data_clean %&gt;%\n  filter(TOTAL == min(TOTAL, na.rm = TRUE))\n\n# Print out the row with the minimum value\nprint(min_row)\n\n\n# A tibble: 146 × 3\n   STATE     COUNTY   TOTAL\n   &lt;chr&gt;     &lt;chr&gt;    &lt;dbl&gt;\n 1 AK        SITKA        1\n 2 AL        BARBOUR      1\n 3 AL        HENRY        1\n 4 AP        APO          1\n 5 AP Total1 /            1\n 6 AR        NEWTON       1\n 7 CA        MONO         1\n 8 CO        BENT         1\n 9 CO        CHEYENNE     1\n10 CO        COSTILLA     1\n# ℹ 136 more rows\n\n\nThe data comes from an Excel file called “StateCounty2012.xls”. After cleaning and processing, it now contains 2931 rows and 3 columns. These data likely come from a survey or statistic about railroad employment.\nThe three columns are as follows:\nThe “STATE” column: This is a character vector, representing various states and other areas in the United States (such as AE and AK), as well as Canada. The mode (most common value) of this column is “character”.\nThe “COUNTY” column: This is also a character vector, representing various counties or regions. Note some special values, for example, when the value of the “STATE” column contains “Total”, the value of this column is replaced with “/”, indicating that this is a summary row. In the row where the value of the “STATE” column is “CANADA”, the value of this column is replaced with “Canada”.\nThe “TOTAL” column: This is a numeric vector, representing the total number of railroad employments in various states and counties. The minimum value of this column is 1, the first quartile is 7, the median is 21, the mean is 87.37, the third quartile is 65, and the maximum value is 8207 (located in Cook County, Illinois)."
  },
  {
    "objectID": "posts/challenge10_instructions.html",
    "href": "posts/challenge10_instructions.html",
    "title": "Challenge 10 Instructions",
    "section": "",
    "text": "The purrr package is a powerful tool for functional programming. It allows the user to apply a single function across multiple objects. It can replace for loops with a more readable (and often faster) simple function call.\nFor example, we can draw n random samples from 10 different distributions using a vector of 10 means.\n\nn &lt;- 100 # sample size\nm &lt;- seq(1,10) # means \nsamps &lt;- map(m,rnorm,n=n) \n\nWe can then use map_dbl to verify that this worked correctly by computing the mean for each sample.\n\nsamps %&gt;%\n  map_dbl(mean)\n\n [1]  0.918026  2.046437  2.997427  4.011996  4.790128  5.974945  7.015693\n [8]  8.206551  9.031059 10.006205\n\n\npurrr is tricky to learn (but beyond useful once you get a handle on it). Therefore, it’s imperative that you complete the purr and map readings before attempting this challenge."
  },
  {
    "objectID": "posts/challenge10_instructions.html#challenge-overview",
    "href": "posts/challenge10_instructions.html#challenge-overview",
    "title": "Challenge 10 Instructions",
    "section": "",
    "text": "The purrr package is a powerful tool for functional programming. It allows the user to apply a single function across multiple objects. It can replace for loops with a more readable (and often faster) simple function call.\nFor example, we can draw n random samples from 10 different distributions using a vector of 10 means.\n\nn &lt;- 100 # sample size\nm &lt;- seq(1,10) # means \nsamps &lt;- map(m,rnorm,n=n) \n\nWe can then use map_dbl to verify that this worked correctly by computing the mean for each sample.\n\nsamps %&gt;%\n  map_dbl(mean)\n\n [1]  0.918026  2.046437  2.997427  4.011996  4.790128  5.974945  7.015693\n [8]  8.206551  9.031059 10.006205\n\n\npurrr is tricky to learn (but beyond useful once you get a handle on it). Therefore, it’s imperative that you complete the purr and map readings before attempting this challenge."
  },
  {
    "objectID": "posts/challenge10_instructions.html#the-challenge",
    "href": "posts/challenge10_instructions.html#the-challenge",
    "title": "Challenge 10 Instructions",
    "section": "The challenge",
    "text": "The challenge\nUse purrr with a function to perform some data science task. What this task is is up to you. It could involve computing summary statistics, reading in multiple datasets, running a random process multiple times, or anything else you might need to do in your work as a data analyst. You might consider using purrr with a function you wrote for challenge 9."
  },
  {
    "objectID": "posts/challenge7_instructions.html",
    "href": "posts/challenge7_instructions.html",
    "title": "Challenge 7 Instructions",
    "section": "",
    "text": "library(tidyverse)\nlibrary(ggplot2)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/challenge7_instructions.html#challenge-overview",
    "href": "posts/challenge7_instructions.html#challenge-overview",
    "title": "Challenge 7 Instructions",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to:\n\nread in a data set, and describe the data set using both words and any supporting information (e.g., tables, etc)\ntidy data (as needed, including sanity checks)\nmutate variables as needed (including sanity checks)\nRecreate at least two graphs from previous exercises, but introduce at least one additional dimension that you omitted before using ggplot functionality (color, shape, line, facet, etc) The goal is not to create unneeded chart ink (Tufte), but to concisely capture variation in additional dimensions that were collapsed in your earlier 2 or 3 dimensional graphs.\n\n\nExplain why you choose the specific graph type\n\n\nIf you haven’t tried in previous weeks, work this week to make your graphs “publication” ready with titles, captions, and pretty axis labels and other viewer-friendly features\n\nR Graph Gallery is a good starting point for thinking about what information is conveyed in standard graph types, and includes example R code. And anyone not familiar with Edward Tufte should check out his fantastic books and courses on data visualizaton.\n(be sure to only include the category tags for the data you use!)"
  },
  {
    "objectID": "posts/challenge7_instructions.html#read-in-data",
    "href": "posts/challenge7_instructions.html#read-in-data",
    "title": "Challenge 7 Instructions",
    "section": "Read in data",
    "text": "Read in data\nRead in one (or more) of the following datasets, using the correct R package and command.\n\neggs ⭐\nabc_poll ⭐⭐\naustralian_marriage ⭐⭐\nhotel_bookings ⭐⭐⭐\nair_bnb ⭐⭐⭐\nus_hh ⭐⭐⭐⭐\nfaostat ⭐⭐⭐⭐⭐\n\n\nBriefly describe the data"
  },
  {
    "objectID": "posts/challenge7_instructions.html#tidy-data-as-needed",
    "href": "posts/challenge7_instructions.html#tidy-data-as-needed",
    "title": "Challenge 7 Instructions",
    "section": "Tidy Data (as needed)",
    "text": "Tidy Data (as needed)\nIs your data already tidy, or is there work to be done? Be sure to anticipate your end result to provide a sanity check, and document your work here.\nAre there any variables that require mutation to be usable in your analysis stream? For example, do you need to calculate new values in order to graph them? Can string values be represented numerically? Do you need to turn any variables into factors and reorder for ease of graphics and visualization?\nDocument your work here."
  },
  {
    "objectID": "posts/challenge7_instructions.html#visualization-with-multiple-dimensions",
    "href": "posts/challenge7_instructions.html#visualization-with-multiple-dimensions",
    "title": "Challenge 7 Instructions",
    "section": "Visualization with Multiple Dimensions",
    "text": "Visualization with Multiple Dimensions"
  },
  {
    "objectID": "posts/LinusJen_challenge1.html",
    "href": "posts/LinusJen_challenge1.html",
    "title": "Challenge 1 Solution - Railroad Employment",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nlibrary(readxl)\n\nknitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)"
  },
  {
    "objectID": "posts/LinusJen_challenge1.html#challenge-overview",
    "href": "posts/LinusJen_challenge1.html#challenge-overview",
    "title": "Challenge 1 Solution - Railroad Employment",
    "section": "Challenge Overview",
    "text": "Challenge Overview\nToday’s challenge is to\n\nread in a dataset, and\ndescribe the dataset using both words and any supporting information (e.g., tables, etc)"
  },
  {
    "objectID": "posts/LinusJen_challenge1.html#read-in-the-data",
    "href": "posts/LinusJen_challenge1.html#read-in-the-data",
    "title": "Challenge 1 Solution - Railroad Employment",
    "section": "Read in the Data",
    "text": "Read in the Data\nFor this homework, I decided to look at the StateCounty2012.xls dataset. I opened the file and noticed that the first three rows are part of the header, so I skipped those lines using the skip argument in the read_xls() function.\n\n\nCode\n# Import data\ndata &lt;- read_xls(\"_data/StateCounty2012.xls\", skip=3)\n\n\n\nData preprocessing\nTo ensure that the data was properly loaded in, I also looked at the top 10 rows of the dataset. I also noticed that there were only 3 columns in the .xls file, but 5 columns in our loaded dataset. To explore further, I applied the is.na() function to all columns, suspecting that the second and fourth columns were loaded improperly and were fully blank. After confirming this, I removed those columns from the dataset. Lastly, I noticed that there were some totals included in our rows, and these occurred when the COUNTY value was empty. We need to be mindful not to remove CANADA though as an observation, even though it doesn’t have a COUNTY value!\n\n\nCode\n# Check head\nprint(head(data, 10))\n\n\n# A tibble: 10 × 5\n   STATE     ...2  COUNTY               ...4  TOTAL\n   &lt;chr&gt;     &lt;lgl&gt; &lt;chr&gt;                &lt;lgl&gt; &lt;dbl&gt;\n 1 AE        NA    APO                  NA        2\n 2 AE Total1 NA    &lt;NA&gt;                 NA        2\n 3 AK        NA    ANCHORAGE            NA        7\n 4 AK        NA    FAIRBANKS NORTH STAR NA        2\n 5 AK        NA    JUNEAU               NA        3\n 6 AK        NA    MATANUSKA-SUSITNA    NA        2\n 7 AK        NA    SITKA                NA        1\n 8 AK        NA    SKAGWAY MUNICIPALITY NA       88\n 9 AK Total  NA    &lt;NA&gt;                 NA      103\n10 AL        NA    AUTAUGA              NA      102\n\n\nCode\n# Find dimensions\nprint(dim(data))\n\n\n[1] 2990    5\n\n\nCode\n# Check if columns are completely blank\nsapply(data, function(x) all(is.na(x)))  # We see that columns 2 and 4 are empty!\n\n\n STATE   ...2 COUNTY   ...4  TOTAL \n FALSE   TRUE  FALSE   TRUE  FALSE \n\n\nCode\n# Check where `COUNTY` is blank\ndata %&gt;% filter(is.na(COUNTY))  # Notice that when `COUNTY` is empty, values are totals are not important\n\n\n\n\n  \n\n\n\nThus, the following chunk of code cleans our dataset to only have US states, counties, and the total railroad employment.\n\n\nCode\n# Remove the 2 columns and when `COUNTY` is empty\ndata_clean &lt;- data %&gt;%\n  select(\"STATE\", \"COUNTY\", \"TOTAL\") %&gt;%\n  filter(!is.na(COUNTY) | (STATE == \"CANADA\"))\n\n# Check dimensions\ndim(data_clean)\n\n\n[1] 2931    3"
  },
  {
    "objectID": "posts/LinusJen_challenge1.html#describe-the-data",
    "href": "posts/LinusJen_challenge1.html#describe-the-data",
    "title": "Challenge 1 Solution - Railroad Employment",
    "section": "Describe the data",
    "text": "Describe the data\n\nOverview / Summary\nFrom the Excel file title, this dataset represents the total railroad employment by state and county for 2012. This data is on a county level, and most likely counts the number of railroad employees for each county, with summations for each state also included in the original .xls file (though it was removed for this dataset). This data might prove valuable if combined with other datasets, such as seeing how the employment per county or state might change over time (if historical data was gathered), checking for relationships being railroad tracks built or existing in each county and the number of employees in 2012 (if data was collected about the railroad track mileage itself), or how railroad employment per county relates back to state statistics, such as population, land size of the state, or unemployment rates of the state (assuming this data is gathered).\nFrom the findings below, we see that there are a total of 256,094 reported railroad employees in 2012. There are a total of 54 unique “counties” that include 2 US territories, Canada, and the District of Columbia. Overall, there seems to be a skew in the distribution of where the railroad employees are, though further geographical analysis needs to be done to make any concrete assumptions.\n\n\nData Exploration\nThis section will provide a deeper dive into the data, and the exact commands used to get the information for the introduction and overview section above.\nFirst, let’s look at the head of the data and the various data types.\n\n\nCode\n# Quick glance of data\nhead(data_clean, 10)\n\n\n\n\n  \n\n\n\nCode\nglimpse(data_clean)\n\n\nRows: 2,931\nColumns: 3\n$ STATE  &lt;chr&gt; \"AE\", \"AK\", \"AK\", \"AK\", \"AK\", \"AK\", \"AK\", \"AL\", \"AL\", \"AL\", \"AL…\n$ COUNTY &lt;chr&gt; \"APO\", \"ANCHORAGE\", \"FAIRBANKS NORTH STAR\", \"JUNEAU\", \"MATANUSK…\n$ TOTAL  &lt;dbl&gt; 2, 7, 2, 3, 2, 1, 88, 102, 143, 1, 25, 154, 13, 29, 45, 13, 9, …\n\n\nCode\n# Check for missingness\ncolSums(is.na(data_clean))\n\n\n STATE COUNTY  TOTAL \n     0      1      0 \n\n\nCode\n# Lastly, check for total number of railroad employees reported\nsum(data_clean$TOTAL)\n\n\n[1] 256094\n\n\nFrom the output above, we see that there are 2,931 observations (rows) with 3 columns of the state name, the county name, and the total number of people employed in that county/state combination. Additionally, we see that there is 1 missing value for the COUNTY column, which is a result of all people employed in Canada being aggregated into one row. Lastly, there were a total of 256,094 total reported railroad employees in 2012.\nNext, let’s look at what states are included in our dataset and the number of counties make up each state.\n\n\nCode\n# Number of states included in dataset\nprint(paste0(\"Number of unique states: \", length(unique(data_clean$STATE))))\n\n\n[1] \"Number of unique states: 54\"\n\n\nCode\n# Check if there are counties with no employes\ndata_clean %&gt;% \n  filter(TOTAL == 0) %&gt;%\n  dim()\n\n\n[1] 0 3\n\n\nCode\n# Check number of counties per state\nnum_counties &lt;- table(data_clean$STATE)\nprint(\"Number of counties \")\n\n\n[1] \"Number of counties \"\n\n\nCode\nnum_counties[order(num_counties, decreasing=TRUE)]\n\n\n\n    TX     GA     KY     MO     IL     IA     KS     NC     IN     VA     TN \n   221    152    119    115    103     99     95     94     92     92     91 \n    NE     OH     MN     MI     MS     OK     AR     WI     AL     FL     PA \n    89     88     86     78     78     73     72     69     67     67     65 \n    LA     NY     CO     CA     MT     WV     SD     ND     SC     WA     ID \n    63     61     57     55     53     53     52     49     46     39     36 \n    OR     NM     UT     MD     WY     NJ     ME     AZ     VT     MA     NV \n    33     29     25     24     22     21     16     15     14     12     12 \n    NH     CT     AK     RI     DE     HI     AE     AP CANADA     DC \n    10      8      6      5      3      3      1      1      1      1 \n\n\nIn the print statement, we see that there are 54 unique “states”. Upon further analysis, we see that this is because Canada is counted as a state in addition to the 50 US states, and several US territories and the US capital are included: “AE” is the armed forces Africa, “AP” is the armed forces in the Pacific, and DC is the District of Columbia. Additionally, we see that ONLY counties with railroad employees are included in this list, so we cannot assume that all counties are included in this dataset.\nFrom our table, We see that Texas, Georgia, and Kentucky are the top three states that have the most counties with railroad employees.\nNext, we’ll look at the railroad employment for each state.\n\n\nCode\nstate_agg &lt;- data_clean %&gt;% \n  group_by(STATE) %&gt;%\n  summarise(total_employment = sum(TOTAL)) %&gt;%\n  arrange(desc(total_employment))\nstate_agg\n\n\n\n\n  \n\n\n\nFrom the table above, we see that Texas, Illinois, and New York make up the top three states with the most people employed working on railroads, while Hawaii, army forces in Africa, and army forces in the Pacific make up the states with the least number of railroad employees. This isn’t shocking. One hypothesis could be that many of the larger and more central states have more railroad employees. This is probably because products need to move across the country, so these states need more railroad tracks (and thus employees to build and maintain these tracks) for goods to move across the country.\nLastly, we’ll look at a simple distribution of the employee data across counties and states.\n\n\nCode\n# 5 number summary for county level\nsummary(data_clean$TOTAL)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.00    7.00   21.00   87.37   65.00 8207.00 \n\n\nCode\n# 5 number summary for state level\nsummary(state_agg$total_employment)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      1    1652    3353    4742    5936   19839 \n\n\nOn the county level, the data is clearly skewed right, as the median of 21 is far below the mean of 87.37. We see that there’s a max of 8,207 railroad employees in one country, which is pretty large. However, there’s also a fair amount of counties with very little employees, as about 25% of counties have 7 or less railroad employees.\nOn the state level, the numbers are far greater for number of railroad employees. The distribution is still skewed right, with the mean of 3,353 still being less than the mean of 4,742. There’s a minimum of 1 railroad employee in a state, and a maximum of 19,839 railroad employees (in Texas, as seen in the previous table / code chunk)."
  },
  {
    "objectID": "posts/AudreyBertin_Challenge1.html",
    "href": "posts/AudreyBertin_Challenge1.html",
    "title": "Challenge 1 - Reading and understanding bird data",
    "section": "",
    "text": "Code\nlibrary(tidyverse)\nknitr::opts_chunk$set(echo = TRUE)\nFor this challenge, I’ll be reading in the following datasets:"
  },
  {
    "objectID": "posts/AudreyBertin_Challenge1.html#birds.csv",
    "href": "posts/AudreyBertin_Challenge1.html#birds.csv",
    "title": "Challenge 1 - Reading and understanding bird data",
    "section": "birds.csv",
    "text": "birds.csv\nFirst, we start with birds.csv:\n\n\nCode\nbirds &lt;- readr::read_csv(\"_data/birds.csv\")\n\n\nA sample of this data can be seen below:\n\n\nCode\nhead(birds)\n\n\n# A tibble: 6 × 14\n  `Domain Code` Domain      `Area Code` Area  `Element Code` Element `Item Code`\n  &lt;chr&gt;         &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;         &lt;dbl&gt;\n1 QA            Live Anima…           2 Afgh…           5112 Stocks         1057\n2 QA            Live Anima…           2 Afgh…           5112 Stocks         1057\n3 QA            Live Anima…           2 Afgh…           5112 Stocks         1057\n4 QA            Live Anima…           2 Afgh…           5112 Stocks         1057\n5 QA            Live Anima…           2 Afgh…           5112 Stocks         1057\n6 QA            Live Anima…           2 Afgh…           5112 Stocks         1057\n# ℹ 7 more variables: Item &lt;chr&gt;, `Year Code` &lt;dbl&gt;, Year &lt;dbl&gt;, Unit &lt;chr&gt;,\n#   Value &lt;dbl&gt;, Flag &lt;chr&gt;, `Flag Description` &lt;chr&gt;\n\n\n\n\nCode\nstr(birds)\n\n\nspc_tbl_ [30,977 × 14] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ Domain Code     : chr [1:30977] \"QA\" \"QA\" \"QA\" \"QA\" ...\n $ Domain          : chr [1:30977] \"Live Animals\" \"Live Animals\" \"Live Animals\" \"Live Animals\" ...\n $ Area Code       : num [1:30977] 2 2 2 2 2 2 2 2 2 2 ...\n $ Area            : chr [1:30977] \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" \"Afghanistan\" ...\n $ Element Code    : num [1:30977] 5112 5112 5112 5112 5112 ...\n $ Element         : chr [1:30977] \"Stocks\" \"Stocks\" \"Stocks\" \"Stocks\" ...\n $ Item Code       : num [1:30977] 1057 1057 1057 1057 1057 ...\n $ Item            : chr [1:30977] \"Chickens\" \"Chickens\" \"Chickens\" \"Chickens\" ...\n $ Year Code       : num [1:30977] 1961 1962 1963 1964 1965 ...\n $ Year            : num [1:30977] 1961 1962 1963 1964 1965 ...\n $ Unit            : chr [1:30977] \"1000 Head\" \"1000 Head\" \"1000 Head\" \"1000 Head\" ...\n $ Value           : num [1:30977] 4700 4900 5000 5300 5500 5800 6600 6290 6300 6000 ...\n $ Flag            : chr [1:30977] \"F\" \"F\" \"F\" \"F\" ...\n $ Flag Description: chr [1:30977] \"FAO estimate\" \"FAO estimate\" \"FAO estimate\" \"FAO estimate\" ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   `Domain Code` = col_character(),\n  ..   Domain = col_character(),\n  ..   `Area Code` = col_double(),\n  ..   Area = col_character(),\n  ..   `Element Code` = col_double(),\n  ..   Element = col_character(),\n  ..   `Item Code` = col_double(),\n  ..   Item = col_character(),\n  ..   `Year Code` = col_double(),\n  ..   Year = col_double(),\n  ..   Unit = col_character(),\n  ..   Value = col_double(),\n  ..   Flag = col_character(),\n  ..   `Flag Description` = col_character()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nAs we can see, we appear to have information on the number of different types of birds in different areas around the world, based on different data sources and estimates. The data appears to be measuring birds in captivity, rather than wild birds.\nWe have 8 categorical variables and 6 numeric. The numeric variables appear to be discrete in that they can only have whole integer values.\nThe different types of birds we are tracking are as follows. We have the most data on chickens and the least on pigeons and other birds.\n\n\nCode\ntable(birds$Item)\n\n\n\n              Chickens                  Ducks Geese and guinea fowls \n                 13074                   6909                   4136 \n  Pigeons, other birds                Turkeys \n                  1165                   5693 \n\n\nThe different type of measurements we have are:\n\n\nCode\ntable(birds$`Flag Description`)\n\n\n\nAggregate, may include official, semi-official, estimated or calculated data \n                                                                        6488 \n                                                          Data not available \n                                                                        1002 \n                                    FAO data based on imputation methodology \n                                                                        1213 \n                                                                FAO estimate \n                                                                       10007 \n                                                               Official data \n                                                                       10773 \n                                                           Unofficial figure \n                                                                        1494 \n\n\nThere appears to be a strange duplicate variable. Both Year and Year Code have identical values.\nThe birds data has 30,977 rows and 14 columns:\n\n\nCode\ndim(birds)\n\n\n[1] 30977    14\n\n\nLooking at missing data, most of our variables have records for every row. There are only two that have missing values, Value and Flag. About 1/30 of the Values are missing and about 1/3 of the flags.\n\n\nCode\ncolSums(is.na(birds))\n\n\n     Domain Code           Domain        Area Code             Area \n               0                0                0                0 \n    Element Code          Element        Item Code             Item \n               0                0                0                0 \n       Year Code             Year             Unit            Value \n               0                0                0             1036 \n            Flag Flag Description \n           10773                0"
  },
  {
    "objectID": "posts/AudreyBertin_Challenge1.html#wild_bird_data.xlsx",
    "href": "posts/AudreyBertin_Challenge1.html#wild_bird_data.xlsx",
    "title": "Challenge 1 - Reading and understanding bird data",
    "section": "wild_bird_data.xlsx",
    "text": "wild_bird_data.xlsx\nWe read this in using read_excel:\n\n\nCode\n# We need to add skip=1 to skip the first row and get rid of duplicate header labels\nwild_birds &lt;- readxl::read_excel(\"_data/wild_bird_data.xlsx\", skip = 1)\n\n\nA sample of this data can be seen below:\n\n\nCode\nhead(wild_birds)\n\n\n# A tibble: 6 × 2\n  `Wet body weight [g]` `Population size`\n                  &lt;dbl&gt;             &lt;dbl&gt;\n1                  5.46           532194.\n2                  7.76          3165107.\n3                  8.64          2592997.\n4                 10.7           3524193.\n5                  7.42           389806.\n6                  9.12           604766.\n\n\n\n\nCode\nstr(wild_birds)\n\n\ntibble [146 × 2] (S3: tbl_df/tbl/data.frame)\n $ Wet body weight [g]: num [1:146] 5.46 7.76 8.64 10.69 7.42 ...\n $ Population size    : num [1:146] 532194 3165107 2592997 3524193 389806 ...\n\n\nAs we can see, we just have two variables here, the wet body weight (in grams) and the size of the population.\nWe can calculate some summary statistics about each of these variables:\n\n\nCode\nsummary(wild_birds)\n\n\n Wet body weight [g] Population size  \n Min.   :   5.459    Min.   :      5  \n 1st Qu.:  18.620    1st Qu.:   1821  \n Median :  69.232    Median :  24353  \n Mean   : 363.694    Mean   : 382874  \n 3rd Qu.: 309.826    3rd Qu.: 198515  \n Max.   :9639.845    Max.   :5093378  \n\n\nBody weight ranges from ~5.5 grams to ~9640 grams. Population size ranges from 5 to 5,093,378, and we can see the means/medians/quartiles above as well.\nThere are 146 rows and none of the data appears to be missing.\n\n\nCode\ndim(wild_birds)\n\n\n[1] 146   2\n\n\n\n\nCode\ncolSums(is.na(wild_birds))\n\n\nWet body weight [g]     Population size \n                  0                   0 \n\n\nIt is difficult just from the dataset itself to understand the context of this data. Is the “Wet body weight [g]” the combined wet weight of the whole population? The average weight of birds in that population? It is unclear how the two columns are related.\nAlso, what type of birds are these? What does a row represent? We would need more information from the source to be able to understand this data further."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Contributors",
    "section": "",
    "text": "Find out more about our DACSS students who contributed to the blog.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAudrey Bertin\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinus Jen\n\n\n\n\n\n\n\n\n\n\n\n\n\nZhongyue Lin\n\n\n\n\n\n\n\nNo matching items"
  }
]